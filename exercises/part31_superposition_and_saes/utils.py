import json
from pathlib import Path
import torch as t
from torch import Tensor
from typing import TYPE_CHECKING
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from jaxtyping import Float
import numpy as np
import einops
from rich import print as rprint
import math
from typing import Literal
import pandas as pd

from matplotlib import pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

Arr = np.ndarray

if TYPE_CHECKING:
    from part31_superposition_and_saes.solutions import Model

red = plt.get_cmap("coolwarm")(0.0)
blue = plt.get_cmap("coolwarm")(1.0)
light_grey = np.array([15 / 16, 15 / 16, 15 / 16, 1.0])
red_grey_blue_cmap = LinearSegmentedColormap.from_list(
    "modified_coolwarm",
    np.vstack([np.linspace(red, light_grey, 128), np.linspace(light_grey, blue, 128)]),
)


def cast_element_to_nested_list(elem, shape: tuple):
    """
    Creates a nested list of shape `shape`, where every element is `elem`.
    Example: ("a", (2, 2)) -> [["a", "a"], ["a", "a"]]
    """
    if len(shape) == 0:
        return elem
    return [cast_element_to_nested_list(elem, shape[1:])] * shape[0]


def plot_correlated_features(batch: Float[Tensor, "batch feats"], title: str):
    go.Figure(
        data=[
            go.Bar(y=batch.squeeze()[:, 0].tolist(), name="Feature 0"),
            go.Bar(y=batch.squeeze()[:, 1].tolist(), name="Feature 1"),
        ],
        layout=dict(
            template="simple_white",
            title=title,
            bargap=0.4,
            bargroupgap=0.0,
            xaxis=dict(tickmode="array", tickvals=list(range(batch.squeeze().shape[0]))),
            xaxis_title="Pairs of features",
            yaxis_title="Feature Values",
            height=400,
            width=1000,
        ),
    ).show()


def sort_W_by_monosemanticity(
    W: Float[Tensor, "feats d_hidden"],
) -> tuple[Float[Tensor, "feats d_hidden"], int]:
    """
    Rearranges the columns of the tensor (i.e. rearranges neurons) in descending order of
    their monosemanticity (where we define monosemanticity as the largest fraction of this
    neuron's norm which is a single feature).

    Also returns the number of "monosemantic features", which we (somewhat arbitrarily)
    define as the fraction being >90% of the total norm.
    """
    norm_by_neuron = W.pow(2).sum(dim=0)
    monosemanticity = W.abs().max(dim=0).values / (norm_by_neuron + 1e-6).sqrt()

    column_order = monosemanticity.argsort(descending=True).tolist()

    n_monosemantic_features = int((monosemanticity.abs() > 0.99).sum().item())

    return W[:, column_order], n_monosemantic_features


def rearrange_full_tensor(
    W: Float[Tensor, "inst d_hidden feats"],
):
    """
    Same as above, but works on W in its original form, and returns a list of
    number of monosemantic features per instance.
    """
    n_monosemantic_features_list = []

    for i, W_inst in enumerate(W):
        W_inst_rearranged, n_monosemantic_features = sort_W_by_monosemanticity(W_inst.T)
        W[i] = W_inst_rearranged.T
        n_monosemantic_features_list.append(n_monosemantic_features)

    return W, n_monosemantic_features_list


def get_viridis_str(v: float) -> str:
    r, g, b, a = plt.get_cmap("viridis")(v)
    r, g, b = int(r * 255), int(g * 255), int(b * 255)
    return f"rgb({r}, {g}, {b})"


def get_viridis(v: float) -> tuple[float, float, float]:
    r, g, b, a = plt.get_cmap("viridis")(v)
    return (r, g, b)


def clamp(x: float, min_val: float, max_val: float) -> float:
    return min(max(x, min_val), max_val)


def plot_features_in_Nd(
    W: Float[Tensor, "inst d_hidden feats"],
    height: int,
    width: int,
    title: str | None = None,
    subplot_titles: list[str] | None = None,
    neuron_plot: bool = False,
):
    n_instances, d_hidden, n_feats = W.shape

    W = W.detach().cpu()

    # Rearrange to align with standard basis
    W, n_monosemantic_features = rearrange_full_tensor(W)

    # Normalize W, i.e. W_normed[inst, i] is normalized i-th feature vector
    W_normed = W / (1e-6 + t.linalg.norm(W, 2, dim=1, keepdim=True))

    # We get interference[i, j] = sum_{j!=i} (W_normed[i] @ W[j]) (ignoring the instance dimension)
    # because then we can calculate superposition by squaring & summing this over j
    interference = einops.einsum(
        W_normed,
        W,
        "instances hidden feats_i, instances hidden feats_j -> instances feats_i feats_j",
    )
    interference[:, range(n_feats), range(n_feats)] = 0

    # Now take the sum, and sqrt (we could just as well not sqrt)
    # Heuristic: polysemanticity is zero if it's orthogonal to all else, one if it's perfectly aligned with any other single vector
    polysemanticity = einops.reduce(
        interference.pow(2),
        "instances feats_i feats_j -> instances feats_i",
        "sum",
    ).sqrt()
    colors = [
        [get_viridis_str(v.item()) for v in polysemanticity_for_this_instance]
        for polysemanticity_for_this_instance in polysemanticity
    ]

    # Get the norms (this is the bar height)
    W_norms = einops.reduce(
        W.pow(2),
        "instances hidden feats -> instances feats",
        "sum",
    ).sqrt()

    # We need W.T @ W for the heatmap (unless this is a neuron plot, then we just use w)
    if not (neuron_plot):
        WtW = einops.einsum(
            W,
            W,
            "instances hidden feats_i, instances hidden feats_j -> instances feats_i feats_j",
        )
        imshow_data = WtW.numpy()
    else:
        imshow_data = einops.rearrange(
            W, "instances hidden feats -> instances feats hidden"
        ).numpy()

    # Get titles (if they exist). Make sure titles only apply to the bar chart in each row
    titles = ["Heatmap of " + ("W" if neuron_plot else "W<sup>T</sup>W")] * n_instances + [
        "Neuron weights<br>stacked bar plot" if neuron_plot else "Feature norms"
    ] * n_instances  # , ||W<sub>i</sub>||
    if subplot_titles is not None:
        for i, st in enumerate(subplot_titles):
            titles[i] = st + "<br>" + titles[i]

    total_height = 0.9 if title is None else 0.8
    if neuron_plot:
        heatmap_height_fraction = clamp(n_feats / (d_hidden + n_feats), 0.5, 0.75)
    else:
        heatmap_height_fraction = 1 - clamp(n_feats / (30 + n_feats), 0.5, 0.75)
    row_heights = [
        total_height * heatmap_height_fraction,
        total_height * (1 - heatmap_height_fraction),
    ]

    n_rows = 2
    n_cols = n_instances

    fig = make_subplots(
        rows=n_rows,
        cols=n_cols,
        vertical_spacing=0.1 if neuron_plot else 0.05,
        row_heights=row_heights,
        subplot_titles=titles,
    )
    for inst in range(n_instances):
        # (1) Add bar charts
        # If it's the non-neuron plot then x = features, y = norms of those features. If it's the
        # neuron plot, our x = neurons (d_hidden), y = the loadings of features on those neurons. In
        # both cases, colors = polysemanticity of features, which we've already computed
        if neuron_plot:
            for feat in range(n_feats):
                fig.add_trace(
                    go.Bar(
                        x=t.arange(d_hidden),
                        y=W[inst, :, feat],
                        marker=dict(color=[colors[inst][feat]] * d_hidden),
                        width=0.9,
                    ),
                    col=1 + inst,
                    row=2,
                )
        else:
            fig.add_trace(
                go.Bar(
                    y=t.arange(n_feats).flip(0),
                    x=W_norms[inst],
                    marker=dict(color=colors[inst]),
                    width=0.9,
                    orientation="h",
                ),
                col=1 + inst,
                row=2,
            )
        # (2) Add heatmap
        # Code is same for neuron plot vs no neuron plot, although data is different: W.T @ W vs W
        fig.add_trace(
            go.Image(
                z=red_grey_blue_cmap((1 + imshow_data[inst]) / 2, bytes=True),
                colormodel="rgba256",
                customdata=imshow_data[inst],
                hovertemplate="""In: %{x}<br>\nOut: %{y}<br>\nWeight: %{customdata:0.2f}""",
            ),
            col=1 + inst,
            row=1,
        )

    if neuron_plot:
        # Stacked plots to allow for all features to be seen
        fig.update_layout(barmode="relative")

        # Weird naming convention for subplots, make sure we have a list of the subplot names for bar charts so we can iterate through them
        n0 = 1 + n_instances
        fig_indices = [str(i) if i != 1 else "" for i in range(n0, n0 + n_instances)]

        for inst in range(n_instances):
            fig["layout"][f"yaxis{fig_indices[inst]}_range"] = [-6, 6]  # type: ignore

            # Add the background colors
            row, col = (2, 1 + inst)
            fig.add_vrect(
                x0=-0.5,
                x1=-0.5 + n_monosemantic_features[inst],
                fillcolor="#440154",
                line_width=0.0,
                opacity=0.2,
                col=col,  # type: ignore
                row=row,  # type: ignore
                layer="below",
            )
            fig.add_vrect(
                x0=-0.5 + n_monosemantic_features[inst],
                x1=-0.5 + d_hidden,
                fillcolor="#fde725",
                line_width=0.0,
                opacity=0.2,
                col=col,  # type: ignore
                row=row,  # type: ignore
                layer="below",
            )

    else:
        # Add annotation of "features" on the y-axis of the bar plot
        fig_indices = [
            str(i) if i != 1 else "" for i in range(n_instances + 1, 2 * n_instances + 1)
        ]
        for inst in range(n_instances):
            fig.add_annotation(
                text="Features âž”",  # âž¤â†’â®•ðŸ¡’âžœ
                xref=f"x{fig_indices[inst]} domain",
                yref=f"y{fig_indices[inst]} domain",
                x=-0.13,
                y=0.99,  # Positioning the annotation outside the first bar plot subfigure
                showarrow=False,
                font=dict(size=12),
                textangle=90,  # Set the text angle to 90 degrees for vertical text
            )

    # Add a horizontal line at the point where n_features = d_hidden (in non-neuron plot). After this point,
    # we must have superposition if we represent all features.
    for annotation in fig.layout.annotations:
        annotation.font.size = 13
    if not neuron_plot:
        fig.add_hline(
            y=n_feats - d_hidden - 0.5,
            line=dict(width=0.5),
            opacity=1.0,
            row=2,  # type: ignore
            annotation_text=f" d_hidden={d_hidden}",
            annotation_position="bottom left",  # "bottom"
            annotation_font_size=11,
        )

    # fig.update_traces(marker_size=1)
    fig.update_layout(
        showlegend=False,
        width=width,
        height=height,
        margin=dict(t=40 if title is None else 110, b=40, l=50, r=40),
        plot_bgcolor="#eee",
        title=title,
        title_y=0.95,
        # template="simple_white",
    )

    fig.update_xaxes(showticklabels=False, showgrid=False)  # visible=False
    fig.update_yaxes(showticklabels=False, showgrid=False)

    fig.show()


def plot_features_in_Nd_discrete(
    W1: Float[Tensor, "inst d_hidden feats"],
    W2: Float[Tensor, "inst feats d_hidden"],
    legend_names: list[str],
    height: int = 600,
    width: int | None = None,
    title: str | None = None,
):
    n_instances, d_hidden, n_feats = W1.shape

    if width is None:
        width = 200 * (n_instances + 1)

    color_list = px.colors.qualitative.D3 + px.colors.qualitative.T10
    assert n_feats <= len(color_list), "Too many features for discrete plot"

    W1 = W1.detach().cpu()
    W2 = W2.detach().cpu()

    titles = [f"Inst={inst}<br>W<sub>1</sub>" for inst in range(n_instances)] + [
        "W<sub>2</sub>" for inst in range(n_instances)
    ]

    fig = make_subplots(rows=2, cols=n_instances, subplot_titles=titles, vertical_spacing=0.1)
    for inst in range(n_instances):
        for feat in range(n_feats):
            fig.add_trace(
                go.Bar(
                    x=t.arange(d_hidden),
                    y=W1[inst, :, feat],
                    marker=dict(color=[color_list[feat]] * d_hidden),
                    showlegend=inst == 0,
                    name=legend_names[feat],
                    width=0.9,
                ),
                col=1 + inst,
                row=1,
            )
            # showlegend=inst==0, name=legend_names[feat]
            fig.add_trace(
                go.Bar(
                    x=t.arange(d_hidden),
                    y=W2[inst, feat, :],
                    marker=dict(color=[color_list[feat]] * d_hidden),
                    showlegend=False,
                    width=0.9,
                ),
                col=1 + inst,
                row=2,
            )

    # Stacked plots to allow for all features to be seen
    fig.update_layout(barmode="relative")

    # Weird naming convention for subplots, make sure we have a list of the subplot names for bar charts so we can iterate through them
    fig_indices = [str(i) if i != 1 else "" for i in range(1, 1 + 2 * n_instances)]
    m = max(W1.abs().max().item(), W2.abs().max().item())
    for inst in range(2 * n_instances):
        fig["layout"][f"yaxis{fig_indices[inst]}_range"] = [-m - 1, m + 1]  # type: ignore

    fig.update_layout(
        legend_title_text="Feature importances",
        width=width,
        height=height,
        margin=dict(t=40 if title is None else 110, b=40, l=50, r=40),
        plot_bgcolor="#eee",
        title=title,
        title_y=0.95,
    )

    fig.update_xaxes(showticklabels=False, showgrid=False)
    fig.update_yaxes(showgrid=False)
    fig.show()


def plot_features_in_2d(
    W: Float[Tensor, "*inst d_hidden feats"],
    colors: Float[Tensor, "inst feats"] | list[str] | list[list[str]] | None = None,
    title: str | None = None,
    subplot_titles: list[str] | None = None,
    n_rows=None,
):
    """
    Visualises superposition in 2D.

    If values is 4D, the first dimension is assumed to be timesteps, and an animation is created.
    """
    if W.ndim == 2:
        W = W.unsqueeze(0)

    n_instances, d_hidden, n_feats = W.shape

    # Convert W into a list of 2D tensors, each of shape [feats, d_hidden=2]
    W = W.detach().cpu()
    W_list: list[Tensor] = [W_instance.T for W_instance in W]

    # Get some plot characteristics
    limits_per_instance = [1.5 for _ in range(n_instances)]
    linewidth, markersize = (1, 4) if (n_feats >= 25) else (1.5, 6)

    # Maybe break onto multiple rows
    if n_rows is None:
        n_rows, n_cols = 1, n_instances
        row_col_tuples = [(0, i) for i in range(n_instances)]
    else:
        n_cols = n_instances // n_rows
        row_col_tuples = [(i // n_cols, i % n_cols) for i in range(n_instances)]

    # Convert colors into a 2D list of strings, with shape [instances, feats]
    if colors is None:
        colors_list = cast_element_to_nested_list("black", (n_instances, n_feats))
    elif isinstance(colors, str):
        colors_list = cast_element_to_nested_list(colors, (n_instances, n_feats))
    elif isinstance(colors, list):
        if isinstance(colors[0], str):
            assert len(colors) == n_feats
            colors_list = [colors for _ in range(n_instances)]
        else:
            assert len(colors) == n_instances and len(colors[0]) == n_feats
            colors_list = colors
    elif isinstance(colors, Tensor):
        assert colors.shape == (n_instances, n_feats)
        colors_list = [[get_viridis(v) for v in color] for color in colors.tolist()]

    # Create a figure and axes, and make sure axs is a 2D array
    fig, axs = plt.subplots(n_rows, n_cols, figsize=(2.5 * n_cols, 2.5 * n_rows))
    axs = np.broadcast_to(axs, (n_rows, n_cols))

    # If there are titles, add more spacing for them
    fig.subplots_adjust(bottom=0.2, top=(0.8 if title else 0.9), left=0.1, right=0.9, hspace=0.5)

    # Initialize lines and markers
    for instance_idx, ((row, col), limits_per_instance) in enumerate(
        zip(row_col_tuples, limits_per_instance)
    ):
        # Get the right axis, and set the limits
        ax = axs[row, col]
        ax.set_xlim(-limits_per_instance, limits_per_instance)
        ax.set_ylim(-limits_per_instance, limits_per_instance)
        ax.set_aspect("equal", adjustable="box")

        # Add all the features for this instance
        for feature_idx in range(n_feats):
            x, y = W_list[instance_idx][feature_idx].tolist()
            color = colors_list[instance_idx][feature_idx]
            ax.plot([0, x], [0, y], color=color, lw=linewidth)[0]
            ax.plot([x, x], [y, y], color=color, marker="o", markersize=markersize)[0]

        # Add titles & subtitles
        if title:
            fig.suptitle(title, fontsize=15)
        if subplot_titles:
            axs[row, col].set_title(subplot_titles[instance_idx], fontsize=12)

    plt.show()


def animate_features_in_2d(
    W: Float[Tensor, "*timesteps inst d_hidden feats"]
    | dict[str, Float[Tensor, "*timesteps inst d_hidden feats"]],
    steps: list[str] | None = None,
    box_size: int = 250,
    fps: int = 50,
    colors: list | Tensor | Literal["resample"] | None = "resample",
    n_seconds_to_highlight: float = 1.0,
    highlight_threshold: float = 0.3,
    title: str = "SAE trained on toy model",
    filename: str | None = None,
) -> str | None:
    """
    Creates an animation of 2D SAE features, as they are learned by the model.

    A few notes:
        - `W` can be passed as a single tensor (= single row) or as a dict of tensors (= multiple
          rows, keys are row titles). If dict & no timesteps, we highlight across rows.
        - `colors` has a bunch of different possible modes:
            - If `colors = "resample"`, that means the features are colored black by default, but
              are colored red for `n_seconds_to_highlight` seconds when their stepwise Euclidean
              distance changes by more than `highlight_threshold`
            - If `colors` is a 1D list, that's interpreted as a list for each instance
            - If `colors` is a 2D tensor, that's interpreted as (n_instances, n_feats), and we
              interpolate with viridis (this is when we use `importance` for our colors)
    """
    if isinstance(W, dict):
        n_rows = len(W)
        n_instances = list(W.values())[0].shape[-3]
        subtitles = [k for k in W.keys() for _ in range(n_instances)]
        W = t.cat([_W.unsqueeze(0) if _W.ndim == 3 else _W for _W in W.values()], dim=1)
    else:
        n_rows = None
        subtitles = None
        W = W.unsqueeze(0) if W.ndim == 3 else W

    W = W.detach().cpu()

    tensor = W.transpose(-1, -2).numpy()
    timestep, plots, datapoints, _ = tensor.shape

    max_size = float(t.topk(W.abs().flatten(), 3)[0][-1].item() * 1.1)
    tickmarks = list(range(-int(max_size), int(max_size) + 1))

    if n_rows is None:
        n_cols = 1
    else:
        assert plots % n_rows == 0
        n_cols = int(plots / n_rows)

    if subtitles is None:
        subtitles = [f"#{i}" for i in range(plots)]
    assert len(subtitles) == plots, (len(subtitles), plots)

    marker_size, line_width = (5, 2) if (datapoints < 25) else (3.5, 1)

    n_steps_to_highlight = int(n_seconds_to_highlight * fps)

    # Handle the different color cases, so we're left with a 3D list of strings
    if colors == "resample":
        if timestep == 1:
            colors_array = cast_element_to_nested_list("black", (timestep, plots, datapoints))
        else:
            assert timestep > 1, "Timestep dimension must be greater than 1 for resample mode."
            euclidean_distances_diff = np.pad(
                np.sqrt(np.sum((tensor[1:] - tensor[:-1]) ** 2, axis=-1)),
                ((1, 0), (0, 0), (0, 0)),
            )
            # Pad the tensor so that we can do the rolling max on the same shape.
            df = pd.DataFrame(data=euclidean_distances_diff.reshape(timestep, -1))
            rolling_max_distances = (
                df.rolling(window=n_steps_to_highlight, min_periods=1)
                .max()
                .values.reshape(euclidean_distances_diff.shape)
            )
            mask = rolling_max_distances > highlight_threshold
            mask[: n_steps_to_highlight + 1, ...] = False  # don't highlight at start
            colors_array = np.where(mask[:, :, :, np.newaxis], "red", "black").tolist()
    elif colors is None:
        colors_array = cast_element_to_nested_list("black", (timestep, plots, datapoints))
    elif isinstance(colors, list):
        assert all(isinstance(c, str) for c in colors), "If list, expected 1D."
        assert len(colors) == datapoints, "Colors list length must match the number of features."
        colors_array = cast_element_to_nested_list(colors, (timestep, plots))
    elif isinstance(colors, Tensor):
        assert colors.shape == (
            plots,
            datapoints,
        ), "colors tensor must be (n_instances, n_feats)"
        assert t.all((colors >= 0) & (colors <= 1)), "color values must be in [0, 1]"
        viridis = plt.get_cmap("viridis")
        colors_array = [
            [
                f"rgb({int(viridis(v)[0]*255)},{int(viridis(v)[1]*255)},{int(viridis(v)[2]*255)})"
                for v in colors[p]
            ]
            for p in range(plots)
        ]
        colors_array = cast_element_to_nested_list(colors_array, (timestep,))

    plot_data = [
        {
            "timestep": ts,
            "plot": p,
            "data": [
                {
                    "x": round(tensor[ts, p, d, 0], 4),
                    "y": round(tensor[ts, p, d, 1], 4),
                    "color": colors_array[ts][p][d],
                }
                for d in range(datapoints)
            ],
        }
        for ts in range(timestep)
        for p in range(plots)
    ]

    # We might not have any play/pause button stuff
    play_button = (
        ""
        if timestep == 1
        else f"""<input type='range' id='slider' min='0' max='{str(timestep - 1)}' value='0' step='1'>
<button class='button' onclick='togglePlayPause()'>Play/Pause</button>"""
    )

    # If we've given step numbers, then add JavaScript to change the plot title telling you how many steps have passed
    change_title_script = (
        ""
        if steps is None
        else f"""
var steps = {json.dumps(steps)}
d3.select('#title').html(`{title}, step ${{steps[timeIdx]}} / ${{steps[steps.length-1]}}`);
"""
    )

    # If we've specified a number of rows, then add CSS to make the grid and enforce this
    plots_style = (
        ""
        if n_rows is None
        else f"#svg-containers {{display: grid; grid-template-columns: repeat({math.ceil(plots / n_rows)}, 1fr); grid-gap: 10px;}}"
    )

    # Create the actual HTML code
    html_template = f"""
<style>
    .svg.plot {{ display: inline-block; width: {box_size}px; height: {box_size}px; }}
    .point {{ fill-opacity: 1.0; }}
    h1 {{ font-family: sans-serif; }}
    span {{ font-family: sans-serif; font-size: 15px; }}
    {plots_style}
</style>

<h1 id="title">{title if title is not None else ""}</h1>
<div id="svg-containers"></div>
{play_button}

<script src="https://d3js.org/d3.v7.min.js"></script>
<script>
    const data = {plot_data};
    const timestep = {timestep};
    const plots = {plots};
    const datapoints = {datapoints};
    const margin = {{top: 10, right: 10, bottom: 30, left: 30}};
    const width = {box_size} - margin.left - margin.right;
    const height = {box_size} - margin.top - margin.bottom;
    const origin = {{x: width / 2, y: height / 2}};
    const xScale = d3.scaleLinear().domain([-{max_size}, {max_size}]).range([0, width]);
    const yScale = d3.scaleLinear().domain([-{max_size}, {max_size}]).range([height, 0]);
    const markerSize = {marker_size};
    const lineWidth = {line_width};
    const subtitles = {subtitles};

    const tooltip = d3.select("body")
        .append("div")
        .style("position", "absolute")
        .style("background", "#eee")
        .style("padding", "6px")
        .style("border", "1px solid black")
        .style("border-radius", "5px")
        .style("pointer-events", "none")
        .style("visibility", "hidden")
        .style("font-size", "12px")
        .style("font-family", "monospace");

    const svgContainers = d3.select("#svg-containers")
        .selectAll("div")
        .data(d3.range(plots))
        .enter()
        .append("div")
        .attr("class", "plot-container")
        .each(function (d, i) {{
            const node = d3.select(this);
            node.append("span").html(subtitles[i]);
            node.append("svg")
                .attr("class", "plot")
                .attr("width", width + margin.left + margin.right)
                .attr("height", height + margin.top + margin.bottom)
                .append("g")
                .attr("transform", `translate(${{margin.left}},${{margin.top}})`);
        }});


    const drawPlot = (plotIdx, timeIdx) => {{
        const plot = d3.selectAll("svg.plot").filter((_, i) => i === plotIdx);
        plot.selectAll("*").remove();

        plot.selectAll(".line")
            .data(data.filter(d => d.plot === plotIdx && d.timestep === timeIdx)[0].data)
            .enter().append("line")
            .attr("class", "line")
            .attr("x1", origin.x)
            .attr("y1", origin.y)
            .attr("x2", d => xScale(d.x))
            .attr("y2", d => yScale(d.y))
            .attr("stroke", d => d.color)
            .attr("stroke-width", lineWidth)

        plot.selectAll(".point")
            .data(data.filter(d => d.plot === plotIdx && d.timestep === timeIdx)[0].data)
            .enter().append("circle")
            .attr("class", "point")
            .attr("r", markerSize)
            .attr("cx", d => xScale(d.x))
            .attr("cy", d => yScale(d.y))
            .attr("fill", d => d.color)
            .each(function(d, i) {{ d.index = i; d.plot = plotIdx; }})
            .on("mouseover", function(event, d) {{
                if ({timestep} > 1) return;
                findConnectedPoints(d, plotIdx).each(function(pointData) {{
                    showTooltip(d3.select(this), pointData, event);
                }});
            }})
            .on("mouseout", function(event, d) {{
                if ({timestep} > 1) return;
                findConnectedPoints(d, plotIdx).each(function(connectedPointData) {{
                    hideTooltip(d3.select(this));
                }});
            }});

        plot.append("g")
            .attr("transform", `translate(0,${{height-1}})`)
            .call(d3.axisBottom(xScale).tickValues({tickmarks}).tickFormat(d3.format(".0f")).tickSize(0))
            .selectAll("text")
            .attr("dy", "11px").attr("font-size", "13px");

        plot.append("g")
            .attr("transform", `translate(1,0)`)
            .call(d3.axisLeft(yScale).tickValues({tickmarks}).tickFormat(d3.format(".0f")).tickSize(0))
            .selectAll("text")
            .attr("dx", "-2px").attr("font-size", "13px");

        plot.append("g")
            .attr("transform", `translate(0,1)`)
            .call(d3.axisTop(xScale).tickValues([]).tickSize(0))
            .selectAll("text").remove();

        plot.append("g")
            .attr("transform", `translate(${{width-1}},0)`)
            .call(d3.axisRight(yScale).tickValues([]).tickSize(0))
            .selectAll("text").remove();

        plot.selectAll(".domain")
            .style("stroke-width", "2px");  // Set stroke width

    }};

    const drawAllPlots = (timeIdx) => {{
        d3.range(plots).forEach(plotIdx => drawPlot(plotIdx, timeIdx));
        {change_title_script}
    }};

    drawAllPlots(0);

    let interval;
    let slider = d3.select('#slider');
    slider.on('input', function() {{ drawAllPlots(+this.value); }});

    function togglePlayPause() {{
        if (interval) {{
            clearInterval(interval);
            interval = null;
        }} else {{
            interval = setInterval(
                function() {{
                    let val = +slider.property('value');
                    slider.property('value', (val + 1) % timestep);
                    drawAllPlots((val + 1) % timestep);
                }},
                {1000 / fps}
            );
        }}
    }}

    function findConnectedPoints(d, plotIdx) {{
        return d3.selectAll(".point")
            .filter(p => p.index === d.index && ({n_rows} ? (p.plot - plotIdx) % {n_cols} == 0 : p.plot == plotIdx))
    }}

    function showTooltip(point, pointData, event) {{
        point.attr("fill", "red").attr("r", markerSize * 1.5);

        const pointCoords = point.node().getBoundingClientRect();

        const tooltip = d3.select("body").append("div")
            .attr("class", "tooltip")
            .style("position", "absolute")
            .style("background", "#eee")
            .style("padding", "6px")
            .style("border", "1px solid black")
            .style("border-radius", "5px")
            .style("pointer-events", "none")
            .style("visibility", "visible")
            .style("font-size", "12px")
            .style("font-family", "monospace")
            .html(`x: ${{pointData.x.toFixed(4)}}, y: ${{pointData.y.toFixed(4)}}`)
            .style("top", (window.scrollY + pointCoords.top - 10) + "px")
            .style("left", (window.scrollX + pointCoords.left + 20) + "px");
    }}

    function hideTooltip(point) {{
        point.attr("fill", d => d.color).attr("r", markerSize);
        d3.selectAll(".tooltip").remove();
    }}
</script>
    """

    if filename is not None:
        assert filename.endswith(".html"), "Filename must end in .html"
        filepath = Path(__file__).parent / filename
        with open(filepath, "w") as file:
            file.write(html_template)
        print(f"Saved animation at {filepath.name!r}")
    else:
        return html_template


# def generate_drift_tensor(timesteps, plots, datapoints):
#     tensor = np.zeros((timesteps, plots, datapoints, 2))
#     for p in range(plots):
#         for d in range(datapoints):
#             point = np.random.rand(2) * 2 - 1
#             tensor[0, p, d] = point
#             for ts in range(1, timesteps):
#                 x = np.random.choice([0.005, 0.5], p=[0.999, 0.001])
#                 random_vector = np.random.randn(2)
#                 random_vector /= np.linalg.norm(random_vector)
#                 tensor[ts, p, d] = (
#                     x * random_vector + math.sqrt(1 - x**2) * tensor[t - 1, p, d]
#                 )
#     return tensor

# def example_animation():
#     tensor = generate_drift_tensor(500, 9, 5)  # 500 timesteps, 9 plots, 5 datapoints
#     tensor = t.from_numpy(tensor)
#     W = tensor.transpose(-1, -2)
#     animate_features_in_2d(
#         W,
#         fps=30,
#         colors="resample",
#         filename="animation-test.html",
#         # n_rows=3,
#     )

# def example_static():
#     n_instances = 5
#     n_feats = 6
#     W = 2 * t.rand(n_instances, 2, n_feats) - 1
#     animate_features_in_2d(
#         W,
#         colors=["green", "red", "blue"] * 2,
#         filename="animation-test.html",
#     )

# def example_dense_static():
#     n_instances = 6
#     n_feats = 30
#     W = 4 * t.rand(n_instances, 2, n_feats) - 2
#     animate_features_in_2d(
#         W,
#         filename="animation-test.html",
#         n_rows=2,
#     )


def frac_active_line_plot(
    frac_active: Float[Tensor, "timesteps inst d_sae"],
    feature_probability: float = 0.01,
    log_freq: int = 50,
    title: str | None = None,
    width: int | None = 1000,
    height: int | None = None,
    y_max: float | None = 0.05,
    avg_window: int | None = None,
):
    if avg_window is not None:
        frac_active_flat = frac_active.detach().cpu().flatten().numpy()
        frac_active_df = pd.DataFrame(data=frac_active_flat).rolling(window=avg_window).mean()
        frac_active = t.from_numpy(frac_active_df.values).reshape(frac_active.shape)

    n_steps, n_instances, d_sae = frac_active.shape

    y_max = y_max if (y_max is not None) else (feature_probability * 3)

    fig = go.Figure(
        layout=dict(
            template="simple_white",
            title=title,
            xaxis_title="Training Step",
            yaxis_title="Fraction of Active Neurons",
            width=width,
            height=height,
            yaxis_range=[0, y_max],
        )
    )

    for inst in range(n_instances):
        for neuron in range(d_sae):
            fig.add_trace(
                go.Scatter(
                    x=list(range(0, log_freq * n_steps, log_freq)),
                    y=frac_active[:, inst, neuron].tolist(),
                    name=f"AE neuron #{neuron}",
                    mode="lines",
                    opacity=0.3,
                    legendgroup=f"Instance #{inst}",
                    legendgrouptitle_text=f"Instance #{inst}",
                )
            )
    fig.add_hline(
        y=feature_probability,
        opacity=1,
        line=dict(color="black", width=2),
        annotation_text="Feature prob",
        annotation_position="bottom left",
        annotation_font_size=14,
    )
    fig.show()


def plot_feature_geometry(model: "Model", dim_fracs=None):
    fig = px.line(
        x=1 / model.feature_probability[:, 0].cpu(),
        y=(model.cfg.d_hidden / (t.linalg.matrix_norm(model.W.detach(), "fro") ** 2)).cpu(),
        log_x=True,
        markers=True,
        template="ggplot2",
        height=600,
        width=1000,
        title="",
    )
    fig.update_layout(title="Number of Hidden Dimensions per Embedded Feature")
    fig.update_xaxes(title="1/(1-S), <-- dense | sparse -->")
    fig.update_yaxes(title="m/||W||_F^2")
    if dim_fracs is not None:
        dim_fracs = dim_fracs.detach().cpu().numpy()
        density = model.feature_probability[:, 0].cpu()

        for a, b in [(1, 2), (2, 3), (2, 5)]:
            val = a / b
            fig.add_hline(
                val,
                line_color="purple",
                opacity=0.2,
                line_width=1,
                annotation=dict(text=f"{a}/{b}"),
            )

        for a, b in [(3, 4), (3, 8), (3, 20)]:
            val = a / b
            fig.add_hline(
                val,
                line_color="purple",
                opacity=0.2,
                line_width=1,
                annotation=dict(text=f"{a}/{b}", x=0.05),
            )

        dx = 0
        for i in range(len(dim_fracs)):
            fracs_ = dim_fracs[i]
            N = fracs_.shape[0]
            xs = 1 / density
            if i != len(dim_fracs) - 1:
                dx = xs[i + 1] - xs[i]
            fig.add_trace(
                go.Scatter(
                    x=1 / density[i] * np.ones(N) + dx * np.random.uniform(-0.1, 0.1, N),
                    y=fracs_,
                    marker=dict(
                        color="black",
                        size=1,
                        opacity=0.5,
                    ),
                    mode="markers",
                )
            )
        fig.update_xaxes(showgrid=False)
        fig.update_yaxes(showgrid=False)
        fig.update_layout(showlegend=False, yaxis_title_text="Dimensionality, or m/||W||_F^2")
    fig.show()


def display_top_sequences(model, top_acts_indices, top_acts_values, tokens):
    s = ""
    for (batch_idx, seq_idx), value in zip(top_acts_indices, top_acts_values):
        # Get the sequence as a string (with some padding on either side of our sequence)
        seq_start = max(seq_idx - 5, 0)
        seq_end = min(seq_idx + 5, tokens.shape[1])
        seq = ""
        # Loop over the sequence, adding each token to the string (highlighting the token with the large activations)
        for i in range(seq_start, seq_end):
            new_str_token = (
                model.to_single_str_token(tokens[batch_idx, i].item())
                .replace("\n", "\\n")
                .replace("<|BOS|>", "|BOS|")
            )
            if i == seq_idx:
                new_str_token = f"[bold u dark_orange]{new_str_token}[/]"
            seq += new_str_token
        # Print the sequence, and the activation value
        s += f'Act = {value:.2f}, Seq = "{seq}"\n'

    rprint(s)
