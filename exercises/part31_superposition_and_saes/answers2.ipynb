{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Literal\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformer_interp\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part31_superposition_and_saes\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "import part31_superposition_and_saes.utils as utils\n",
    "import part31_superposition_and_saes.tests as tests\n",
    "from plotly_utils import line, imshow\n",
    "\n",
    "device = t.device(\n",
    "    \"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_lr(step, steps):\n",
    "    return (1 - (step / steps))\n",
    "\n",
    "def constant_lr(*_):\n",
    "    return 1.0\n",
    "\n",
    "def cosine_decay_lr(step, steps):\n",
    "    return np.cos(0.5 * np.pi * step / (steps - 1))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # We optimize n_inst models in a single training loop to let us sweep over sparsity or importance\n",
    "    # curves efficiently. You should treat the number of instances `n_inst` like a batch dimension, \n",
    "    # but one which is built into our training setup. Ignore the latter 3 arguments for now, they'll\n",
    "    # return in later exercises.\n",
    "    n_inst: int\n",
    "    n_features: int = 5\n",
    "    d_hidden: int = 2\n",
    "    n_correlated_pairs: int = 0\n",
    "    n_anticorrelated_pairs: int = 0\n",
    "    feat_mag_distn: Literal[\"unif\", \"jump\"] = \"unif\"\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    W: Float[Tensor, \"inst d_hidden feats\"]\n",
    "    b_final: Float[Tensor, \"inst feats\"]\n",
    "\n",
    "    # Our linear map (for a single instance) is x -> ReLU(W.T @ W @ x + b_final)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        feature_probability: float | Tensor = 0.01,\n",
    "        importance: float | Tensor = 1.0,\n",
    "        device=device,\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if isinstance(feature_probability, float):\n",
    "            feature_probability = t.tensor(feature_probability)\n",
    "        self.feature_probability = feature_probability.to(device).broadcast_to(\n",
    "            (cfg.n_inst, cfg.n_features)\n",
    "        )\n",
    "        if isinstance(importance, float):\n",
    "            importance = t.tensor(importance)\n",
    "        self.importance = importance.to(device).broadcast_to((cfg.n_inst, cfg.n_features))\n",
    "\n",
    "        self.W = nn.Parameter(\n",
    "            nn.init.xavier_normal_(t.empty((cfg.n_inst, cfg.d_hidden, cfg.n_features)))\n",
    "        )\n",
    "        self.b_final = nn.Parameter(t.zeros((cfg.n_inst, cfg.n_features)))\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: Float[Tensor, \"... inst feats\"],\n",
    "    ) -> Float[Tensor, \"... inst feats\"]:\n",
    "        h = einops.einsum(self.W, features,\n",
    "            \"inst d_hidden feats,... inst feats ->... inst d_hidden\")\n",
    "        h1 = einops.einsum(self.W.transpose(1,2),h,\n",
    "            \"inst feats d_hidden,... inst d_hidden ->... inst feats\")\n",
    "        out = F.relu(h1 + self.b_final)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def generate_correlated_features(\n",
    "        self, batch_size: int, n_correlated_pairs: int\n",
    "    ) -> Float[Tensor, \"batch inst 2*n_correlated_pairs\"]:\n",
    "        \"\"\"\n",
    "        Generates a batch of correlated features. For each pair `batch[i, j, [2k, 2k+1]]`, one of\n",
    "        them is non-zero if and only if the other is non-zero.\n",
    "\n",
    "        This solution works by creating a boolean mask of shape [batch inst n_correlated_pairs]\n",
    "        which represents whether the feature set is present, then repeating that mask across feature\n",
    "        pairs.\n",
    "        \"\"\"\n",
    "        assert t.all((self.feature_probability == self.feature_probability[:, [0]]))\n",
    "        p = self.feature_probability[:, [0]]  # shape (n_inst, 1)\n",
    "\n",
    "        feat_mag = t.rand(\n",
    "            (batch_size, self.cfg.n_inst, 2 * n_correlated_pairs), device=self.W.device\n",
    "        )\n",
    "        feat_set_seeds = t.rand(\n",
    "            (batch_size, self.cfg.n_inst, n_correlated_pairs), device=self.W.device\n",
    "        )\n",
    "        feat_set_is_present = feat_set_seeds <= p\n",
    "        feat_is_present = einops.repeat(\n",
    "            feat_set_is_present,\n",
    "            \"batch instances features -> batch instances (features pair)\",\n",
    "            pair=2,\n",
    "        )\n",
    "        return t.where(feat_is_present, feat_mag, 0.0)\n",
    "\n",
    "    def generate_anticorrelated_features(\n",
    "        self, batch_size: int, n_anticorrelated_pairs: int\n",
    "    ) -> Float[Tensor, \"batch inst 2*n_anticorrelated_pairs\"]:\n",
    "        \"\"\"\n",
    "        Generates a batch of anti-correlated features. For each pair `batch[i, j, [2k, 2k+1]]`, each\n",
    "        of them can only be non-zero if the other one is zero.\n",
    "\n",
    "        There are at least 2 possible ways you could do this:\n",
    "            (1) Exactly one of batch[i, j, [2k, 2k+1]] is present with probability 2p, and in this\n",
    "                event we choose which of these two is present randomly.\n",
    "            (2) batch[i, j, 2k] is present with probability p, and batch[i, j, 2k+1] is present with\n",
    "                probability p / (1 - p) if and only if batch[i, j, 2k] is present.\n",
    "\n",
    "        This solution uses (2), but both are valid.\n",
    "        \"\"\"\n",
    "        assert t.all((self.feature_probability == self.feature_probability[:, [0]]))\n",
    "        p = self.feature_probability[:, [0]]  # shape (n_inst, 1)\n",
    "\n",
    "        assert p.max().item() <= 0.5, \"For anticorrelated features, must have 2p < 1\"\n",
    "\n",
    "        feat_mag = t.rand(\n",
    "            (batch_size, self.cfg.n_inst, 2 * n_anticorrelated_pairs), device=self.W.device\n",
    "        )\n",
    "        even_feat_seeds, odd_feat_seeds = t.rand(\n",
    "            (2, batch_size, self.cfg.n_inst, n_anticorrelated_pairs),\n",
    "            device=self.W.device,\n",
    "        )\n",
    "        even_feat_is_present = even_feat_seeds <= p\n",
    "        odd_feat_is_present = (even_feat_seeds > p) & (odd_feat_seeds <= p / (1 - p))\n",
    "        feat_is_present = einops.rearrange(\n",
    "            t.stack([even_feat_is_present, odd_feat_is_present], dim=0),\n",
    "            \"pair batch instances features -> batch instances (features pair)\",\n",
    "        )\n",
    "        return t.where(feat_is_present, feat_mag, 0.0)\n",
    "\n",
    "    def generate_uncorrelated_features(self, batch_size: int, n_uncorrelated: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Generates a batch of uncorrelated features.\n",
    "        \"\"\"\n",
    "        if n_uncorrelated == self.cfg.n_features:\n",
    "            p = self.feature_probability\n",
    "        else:\n",
    "            assert t.all((self.feature_probability == self.feature_probability[:, [0]]))\n",
    "            p = self.feature_probability[:, [0]]  # shape (n_inst, 1)\n",
    "\n",
    "        feat_mag = t.rand((batch_size, self.cfg.n_inst, n_uncorrelated), device=self.W.device)\n",
    "        feat_seeds = t.rand((batch_size, self.cfg.n_inst, n_uncorrelated), device=self.W.device)\n",
    "        return t.where(feat_seeds <= p, feat_mag, 0.0)\n",
    "        \n",
    "    def generate_batch(self, batch_size) -> Float[Tensor, \"batch inst feats\"]:\n",
    "        \"\"\"\n",
    "        Generates a batch of data, with optional correlated & anticorrelated features.\n",
    "        \"\"\"\n",
    "        n_corr_pairs = self.cfg.n_correlated_pairs\n",
    "        n_anti_pairs = self.cfg.n_anticorrelated_pairs\n",
    "        n_uncorr = self.cfg.n_features - 2 * n_corr_pairs - 2 * n_anti_pairs\n",
    "\n",
    "        data = []\n",
    "        if n_corr_pairs > 0:\n",
    "            data.append(self.generate_correlated_features(batch_size, n_corr_pairs))\n",
    "        if n_anti_pairs > 0:\n",
    "            data.append(self.generate_anticorrelated_features(batch_size, n_anti_pairs))\n",
    "        if n_uncorr > 0:\n",
    "            data.append(self.generate_uncorrelated_features(batch_size, n_uncorr))\n",
    "        batch = t.cat(data, dim=-1)\n",
    "        return batch\n",
    "\n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        out: Float[Tensor, \"batch inst feats\"],\n",
    "        batch: Float[Tensor, \"batch inst feats\"],\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        \"\"\"\n",
    "        Calculates the loss for a given batch (as a scalar tensor), using this loss described in the\n",
    "        Toy Models of Superposition paper:\n",
    "\n",
    "            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n",
    "\n",
    "        Note, `self.importance` is guaranteed to broadcast with the shape of `out` and `batch`.\n",
    "        \"\"\"\n",
    "        # You'll fill this in later\n",
    "        error = self.importance * ( (out-batch)**2 )\n",
    "        loss = einops.reduce(error,\n",
    "                             \"batch inst feats->inst\", \"mean\").sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        batch_size: int = 1024,\n",
    "        steps: int = 10_000,\n",
    "        log_freq: int = 50,\n",
    "        lr: float = 1e-3,\n",
    "        lr_scale: Callable[[int, int], float] = constant_lr,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Optimizes the model using the given hyperparameters.\n",
    "        \"\"\"\n",
    "        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n",
    "\n",
    "        progress_bar = tqdm(range(steps))\n",
    "\n",
    "        for step in progress_bar:\n",
    "            # Update learning rate\n",
    "            step_lr = lr * lr_scale(step, steps)\n",
    "            for group in optimizer.param_groups:\n",
    "                group[\"lr\"] = step_lr\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            batch = self.generate_batch(batch_size)\n",
    "            out = self(batch)\n",
    "            loss = self.calculate_loss(out, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Display progress bar\n",
    "            if step % log_freq == 0 or (step + 1 == steps):\n",
    "                progress_bar.set_postfix(loss=loss.item() / self.cfg.n_inst, lr=step_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(n_inst=30, n_features=4, d_hidden=2, n_correlated_pairs=1, n_anticorrelated_pairs=1)\n",
    "\n",
    "\n",
    "feature_probability = 10 ** -t.linspace(0.5, 1, cfg.n_inst).to(device)\n",
    "\n",
    "model = Model(cfg=cfg, device=device, feature_probability=feature_probability[:, None])\n",
    "\n",
    "# Generate a batch of 4 features: first 2 are correlated, second 2 are anticorrelated\n",
    "batch = model.generate_batch(batch_size=100_000)\n",
    "corr0, corr1, anticorr0, anticorr1 = batch.unbind(dim=-1)\n",
    "\n",
    "assert ((corr0 != 0) == (corr1 != 0)).all(), \"Correlated features should be active together\"\n",
    "assert (\n",
    "    ((corr0 != 0).float().mean(0) - feature_probability).abs().mean() < 0.002\n",
    "), \"Each correlated feature should be active with probability `feature_probability`\"\n",
    "\n",
    "assert (\n",
    "    (anticorr0 != 0) & (anticorr1 != 0)\n",
    ").int().sum().item() == 0, \"Anticorrelated features should never be active together\"\n",
    "assert (\n",
    "    ((anticorr0 != 0).float().mean(0) - feature_probability).abs().mean() < 0.002\n",
    "), \"Each anticorrelated feature should be active with probability `feature_probability`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of 4 features: first 2 are correlated, second 2 are anticorrelated\n",
    "batch = model.generate_batch(batch_size=1)\n",
    "correlated_feature_batch, anticorrelated_feature_batch = batch.split(2, dim=-1)\n",
    "\n",
    "# Plot correlated features\n",
    "utils.plot_correlated_features(\n",
    "    correlated_feature_batch, title=\"Correlated feature pairs: should always co-occur\"\n",
    ")\n",
    "utils.plot_correlated_features(\n",
    "    anticorrelated_feature_batch, title=\"Anti-correlated feature pairs: should never co-occur\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(n_inst=5, n_features=4, d_hidden=2, n_correlated_pairs=0,n_anticorrelated_pairs=0)\n",
    "\n",
    "# All same importance, very low feature probabilities (ranging from 5% down to 0.25%)\n",
    "importance = t.ones(cfg.n_features, dtype=t.float, device=device)\n",
    "feature_probability = 400 ** -t.linspace(0.5, 1, cfg.n_inst)\n",
    "\n",
    "model = Model(\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    importance=importance[None, :],\n",
    "    feature_probability=feature_probability[:, None],\n",
    ")\n",
    "model.optimize(steps=10_000)\n",
    "\n",
    "utils.plot_features_in_2d(\n",
    "    model.W,\n",
    "    colors=[\"blue\"] * 2 + [\"limegreen\"] * 2,\n",
    "    title=\"Correlated feature sets are represented in local orthogonal bases\",\n",
    "    subplot_titles=[f\"1 - S = {i:.3f}\" for i in feature_probability],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
