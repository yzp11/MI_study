{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Literal\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformer_interp\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part31_superposition_and_saes\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "import part31_superposition_and_saes.utils as utils\n",
    "import part31_superposition_and_saes.tests as tests\n",
    "from plotly_utils import line, imshow\n",
    "\n",
    "device = t.device(\n",
    "    \"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(device)\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(2)\n",
    "\n",
    "W = t.randn(2, 5)\n",
    "W_normed = W / W.norm(dim=0, keepdim=True)\n",
    "\n",
    "imshow(W_normed.T @ W_normed, title=\"Cosine similarities of each pair of 2D feature embeddings\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_features_in_2d(W_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_model` passed!\n"
     ]
    }
   ],
   "source": [
    "def linear_lr(step, steps):\n",
    "    return (1 - (step / steps))\n",
    "\n",
    "def constant_lr(*_):\n",
    "    return 1.0\n",
    "\n",
    "def cosine_decay_lr(step, steps):\n",
    "    return np.cos(0.5 * np.pi * step / (steps - 1))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # We optimize n_inst models in a single training loop to let us sweep over sparsity or importance\n",
    "    # curves efficiently. You should treat the number of instances `n_inst` like a batch dimension, \n",
    "    # but one which is built into our training setup. Ignore the latter 3 arguments for now, they'll\n",
    "    # return in later exercises.\n",
    "    n_inst: int\n",
    "    n_features: int = 5\n",
    "    d_hidden: int = 2\n",
    "    n_correlated_pairs: int = 0\n",
    "    n_anticorrelated_pairs: int = 0\n",
    "    feat_mag_distn: Literal[\"unif\", \"jump\"] = \"unif\"\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    W: Float[Tensor, \"inst d_hidden feats\"]\n",
    "    b_final: Float[Tensor, \"inst feats\"]\n",
    "\n",
    "    # Our linear map (for a single instance) is x -> ReLU(W.T @ W @ x + b_final)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        feature_probability: float | Tensor = 0.01,\n",
    "        importance: float | Tensor = 1.0,\n",
    "        device=device,\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if isinstance(feature_probability, float):\n",
    "            feature_probability = t.tensor(feature_probability)\n",
    "        self.feature_probability = feature_probability.to(device).broadcast_to(\n",
    "            (cfg.n_inst, cfg.n_features)\n",
    "        )\n",
    "        if isinstance(importance, float):\n",
    "            importance = t.tensor(importance)\n",
    "        self.importance = importance.to(device).broadcast_to((cfg.n_inst, cfg.n_features))\n",
    "\n",
    "        self.W = nn.Parameter(\n",
    "            nn.init.xavier_normal_(t.empty((cfg.n_inst, cfg.d_hidden, cfg.n_features)))\n",
    "        )\n",
    "        self.b_final = nn.Parameter(t.zeros((cfg.n_inst, cfg.n_features)))\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: Float[Tensor, \"... inst feats\"],\n",
    "    ) -> Float[Tensor, \"... inst feats\"]:\n",
    "        h = einops.einsum(self.W, features,\n",
    "            \"inst d_hidden feats,... inst feats ->... inst d_hidden\")\n",
    "        h1 = einops.einsum(self.W.transpose(1,2),h,\n",
    "            \"inst feats d_hidden,... inst d_hidden ->... inst feats\")\n",
    "        out = F.relu(h1 + self.b_final)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def generate_batch(self, batch_size) -> Float[Tensor, \"batch inst feats\"]:\n",
    "        \"\"\"\n",
    "        Generates a batch of data.\n",
    "        \"\"\"\n",
    "        # You'll fill this in later\n",
    "\n",
    "        data_size = (batch_size, self.cfg.n_inst, self.cfg.n_features)\n",
    "        data = t.rand(data_size, device= self.W.device)\n",
    "        pro = t.rand(data_size, device= self.W.device)\n",
    "\n",
    "        return t.where(pro<self.feature_probability, data, 0.0)\n",
    "        \n",
    "\n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        out: Float[Tensor, \"batch inst feats\"],\n",
    "        batch: Float[Tensor, \"batch inst feats\"],\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        \"\"\"\n",
    "        Calculates the loss for a given batch (as a scalar tensor), using this loss described in the\n",
    "        Toy Models of Superposition paper:\n",
    "\n",
    "            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n",
    "\n",
    "        Note, `self.importance` is guaranteed to broadcast with the shape of `out` and `batch`.\n",
    "        \"\"\"\n",
    "        # You'll fill this in later\n",
    "        error = self.importance * ( (out-batch)**2 )\n",
    "        loss = einops.reduce(error,\n",
    "                             \"batch inst feats->inst\", \"mean\").sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        batch_size: int = 1024,\n",
    "        steps: int = 10_000,\n",
    "        log_freq: int = 50,\n",
    "        lr: float = 1e-3,\n",
    "        lr_scale: Callable[[int, int], float] = constant_lr,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Optimizes the model using the given hyperparameters.\n",
    "        \"\"\"\n",
    "        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n",
    "\n",
    "        progress_bar = tqdm(range(steps))\n",
    "\n",
    "        for step in progress_bar:\n",
    "            # Update learning rate\n",
    "            step_lr = lr * lr_scale(step, steps)\n",
    "            for group in optimizer.param_groups:\n",
    "                group[\"lr\"] = step_lr\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            batch = self.generate_batch(batch_size)\n",
    "            out = self(batch)\n",
    "            loss = self.calculate_loss(out, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Display progress bar\n",
    "            if step % log_freq == 0 or (step + 1 == steps):\n",
    "                progress_bar.set_postfix(loss=loss.item() / self.cfg.n_inst, lr=step_lr)\n",
    "\n",
    "\n",
    "tests.test_model(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.test_generate_batch(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.test_calculate_loss(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(n_inst=8, n_features=5, d_hidden=2)\n",
    "\n",
    "# importance varies within features for each instance\n",
    "importance = (0.9 ** t.arange(cfg.n_features))\n",
    "\n",
    "# sparsity is the same for all features in a given instance, but varies over instances\n",
    "feature_probability = (50 ** -t.linspace(0, 1, cfg.n_inst))\n",
    "\n",
    "line(importance, width=600, height=400, title=\"Importance of each feature (same over all instances)\", labels={\"y\": \"Feature importance\", \"x\": \"Feature\"})\n",
    "line(feature_probability, width=600, height=400, title=\"Feature probability (varied over instances)\", labels={\"y\": \"Probability\", \"x\": \"Instance\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    importance=importance[None, :],\n",
    "    feature_probability=feature_probability[:, None],\n",
    ")\n",
    "model.optimize(steps=10_000)\n",
    "\n",
    "utils.plot_features_in_2d(\n",
    "    model.W,\n",
    "    colors=model.importance,\n",
    "    title=f\"Superposition: {cfg.n_features} features represented in 2D space\",\n",
    "    subplot_titles=[f\"1 - S = {i:.3f}\" for i in feature_probability.squeeze()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with t.inference_mode():\n",
    "    batch = model.generate_batch(250)\n",
    "    h = einops.einsum(\n",
    "        batch, model.W, \"batch inst feats, inst hidden feats -> inst hidden batch\"\n",
    "    )\n",
    "\n",
    "utils.plot_features_in_2d(h, title=\"Hidden state representation of a random batch of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(n_inst=10, n_features=100, d_hidden=20)\n",
    "\n",
    "importance = 100 ** -t.linspace(0, 1, cfg.n_features)\n",
    "feature_probability = 20 ** -t.linspace(0, 1, cfg.n_inst)\n",
    "\n",
    "line(importance, width=600, height=400, title=\"Importance of each feature (same over all instances)\", labels={\"y\": \"Feature importance\", \"x\": \"Feature\"})\n",
    "line(feature_probability, width=600, height=400, title=\"Feature probability (varied over instances)\", labels={\"y\": \"Probability\", \"x\": \"Instance\"})\n",
    "\n",
    "model = Model(\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    importance=importance[None, :],\n",
    "    feature_probability=feature_probability[:, None],\n",
    ")\n",
    "model.optimize(steps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_features_in_Nd(\n",
    "    model.W,\n",
    "    height=800,\n",
    "    width=1600,\n",
    "    title=\"ReLU output model: n_features = 80, d_hidden = 20, I<sub>i</sub> = 0.9<sup>i</sup>\",\n",
    "    subplot_titles=[f\"Feature prob = {i:.3f}\" for i in feature_probability],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mConfig\u001b[49m(n_inst\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, n_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, d_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Now we vary feature probability within features (but same for all instances)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# we make all probs 0.05, except for the first feature which has smaller probability\u001b[39;00m\n\u001b[1;32m      5\u001b[0m feature_probability \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mfull((cfg\u001b[38;5;241m.\u001b[39mn_inst, cfg\u001b[38;5;241m.\u001b[39mn_features), \u001b[38;5;241m0.05\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"
     ]
    }
   ],
   "source": [
    "cfg = Config(n_inst=16, n_features=5, d_hidden=2)\n",
    "\n",
    "# Now we vary feature probability within features (but same for all instances)\n",
    "# we make all probs 0.05, except for the first feature which has smaller probability\n",
    "feature_probability = t.full((cfg.n_inst, cfg.n_features), 0.05)\n",
    "feature_probability[:, 0] *= t.linspace(0, 1, cfg.n_inst + 1)[1:].flip(0)\n",
    "\n",
    "model = Model(\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    feature_probability=feature_probability,\n",
    ")\n",
    "model.optimize(steps=25_000)\n",
    "\n",
    "# To help distinguish colors, we normalize to use the full color range\n",
    "colors = model.feature_probability * (1 / model.feature_probability.max())\n",
    "\n",
    "utils.plot_features_in_2d(\n",
    "    model.W,\n",
    "    colors=colors,\n",
    "    title=f\"Superposition: {cfg.n_features} features represented in 2D space (lighter colors = larger feature probabilities)\",\n",
    "    subplot_titles=[f\"1 - S = 0.05 * {i:.2f}\" for i in t.linspace(0, 1, cfg.n_inst).flip(0)],\n",
    "    n_rows=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
