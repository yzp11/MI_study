{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Literal\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformer_interp\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part31_superposition_and_saes\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "import part31_superposition_and_saes.utils as utils\n",
    "import part31_superposition_and_saes.tests as tests\n",
    "from plotly_utils import line, imshow\n",
    "\n",
    "device = t.device(\n",
    "    \"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "t.manual_seed(2)\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_lr(step, steps):\n",
    "    return (1 - (step / steps))\n",
    "\n",
    "def constant_lr(*_):\n",
    "    return 1.0\n",
    "\n",
    "def cosine_decay_lr(step, steps):\n",
    "    return np.cos(0.5 * np.pi * step / (steps - 1))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # We optimize n_inst models in a single training loop to let us sweep over sparsity or importance\n",
    "    # curves efficiently. You should treat the number of instances `n_inst` like a batch dimension, \n",
    "    # but one which is built into our training setup. Ignore the latter 3 arguments for now, they'll\n",
    "    # return in later exercises.\n",
    "    n_inst: int\n",
    "    n_features: int = 5\n",
    "    d_hidden: int = 2\n",
    "    n_correlated_pairs: int = 0\n",
    "    n_anticorrelated_pairs: int = 0\n",
    "    feat_mag_distn: Literal[\"unif\", \"jump\"] = \"unif\"\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    W: Float[Tensor, \"inst d_hidden feats\"]\n",
    "    b_final: Float[Tensor, \"inst feats\"]\n",
    "\n",
    "    # Our linear map (for a single instance) is x -> ReLU(W.T @ W @ x + b_final)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        feature_probability: float | Tensor = 0.01,\n",
    "        importance: float | Tensor = 1.0,\n",
    "        device=device,\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if isinstance(feature_probability, float):\n",
    "            feature_probability = t.tensor(feature_probability)\n",
    "        self.feature_probability = feature_probability.to(device).broadcast_to(\n",
    "            (cfg.n_inst, cfg.n_features)\n",
    "        )\n",
    "        if isinstance(importance, float):\n",
    "            importance = t.tensor(importance)\n",
    "        self.importance = importance.to(device).broadcast_to((cfg.n_inst, cfg.n_features))\n",
    "\n",
    "        self.W = nn.Parameter(\n",
    "            nn.init.xavier_normal_(t.empty((cfg.n_inst, cfg.d_hidden, cfg.n_features)))\n",
    "        )\n",
    "        self.b_final = nn.Parameter(t.zeros((cfg.n_inst, cfg.n_features)))\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: Float[Tensor, \"... inst feats\"],\n",
    "    ) -> Float[Tensor, \"... inst feats\"]:\n",
    "        h = einops.einsum(self.W, features,\n",
    "            \"inst d_hidden feats,... inst feats ->... inst d_hidden\")\n",
    "        h1 = einops.einsum(self.W.transpose(1,2),h,\n",
    "            \"inst feats d_hidden,... inst d_hidden ->... inst feats\")\n",
    "        out = F.relu(h1 + self.b_final)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def generate_batch(self, batch_size) -> Float[Tensor, \"batch inst feats\"]:\n",
    "        \"\"\"\n",
    "        Generates a batch of data.\n",
    "        \"\"\"\n",
    "        # You'll fill this in later\n",
    "\n",
    "        data_size = (batch_size, self.cfg.n_inst, self.cfg.n_features)\n",
    "        data = t.rand(data_size, device= self.W.device)\n",
    "        pro = t.rand(data_size, device= self.W.device)\n",
    "\n",
    "        return t.where(pro<self.feature_probability, data, 0.0)\n",
    "        \n",
    "\n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        out: Float[Tensor, \"batch inst feats\"],\n",
    "        batch: Float[Tensor, \"batch inst feats\"],\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        \"\"\"\n",
    "        Calculates the loss for a given batch (as a scalar tensor), using this loss described in the\n",
    "        Toy Models of Superposition paper:\n",
    "\n",
    "            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n",
    "\n",
    "        Note, `self.importance` is guaranteed to broadcast with the shape of `out` and `batch`.\n",
    "        \"\"\"\n",
    "        # You'll fill this in later\n",
    "        error = self.importance * ( (out-batch)**2 )\n",
    "        loss = einops.reduce(error,\n",
    "                             \"batch inst feats->inst\", \"mean\").sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        batch_size: int = 1024,\n",
    "        steps: int = 10_000,\n",
    "        log_freq: int = 50,\n",
    "        lr: float = 1e-3,\n",
    "        lr_scale: Callable[[int, int], float] = constant_lr,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Optimizes the model using the given hyperparameters.\n",
    "        \"\"\"\n",
    "        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n",
    "\n",
    "        progress_bar = tqdm(range(steps))\n",
    "\n",
    "        for step in progress_bar:\n",
    "            # Update learning rate\n",
    "            step_lr = lr * lr_scale(step, steps)\n",
    "            for group in optimizer.param_groups:\n",
    "                group[\"lr\"] = step_lr\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            batch = self.generate_batch(batch_size)\n",
    "            out = self(batch)\n",
    "            loss = self.calculate_loss(out, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Display progress bar\n",
    "            if step % log_freq == 0 or (step + 1 == steps):\n",
    "                progress_bar.set_postfix(loss=loss.item() / self.cfg.n_inst, lr=step_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronModel(Model):\n",
    "    def forward(\n",
    "        self,\n",
    "        features: Float[Tensor, \"... instances features\"]\n",
    "    ) -> Float[Tensor, \"... instances features\"]:\n",
    "        h = einops.einsum(self.W, features,\n",
    "            \"inst d_hidden feats,... inst feats ->... inst d_hidden\")\n",
    "        h = F.relu(h)\n",
    "        h1 = einops.einsum(self.W.transpose(1,2),h,\n",
    "            \"inst feats d_hidden,... inst d_hidden ->... inst feats\")\n",
    "        out = F.relu(h1 + self.b_final)\n",
    "        return out\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_neuron_model(NeuronModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(n_inst=7, n_features=10, d_hidden=5)\n",
    "\n",
    "importance = 0.75 ** t.arange(1, 1 + cfg.n_features)\n",
    "feature_probability = t.tensor([0.75, 0.35, 0.15, 0.1, 0.06, 0.02, 0.01])\n",
    "\n",
    "model = NeuronModel(\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    importance=importance[None, :],\n",
    "    feature_probability=feature_probability[:, None],\n",
    ")\n",
    "model.optimize(steps=10_000)\n",
    "\n",
    "utils.plot_features_in_Nd(\n",
    "    model.W,\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    subplot_titles=[f\"1 - S = {i:.2f}\" for i in feature_probability.squeeze()],\n",
    "    title=f\"Neuron model: {cfg.n_features=}, {cfg.d_hidden=}, I<sub>i</sub> = 0.75<sup>i</sup>\",\n",
    "    neuron_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronComputationModel(Model):\n",
    "    W1: Float[Tensor, \"inst d_hidden feats\"]\n",
    "    W2: Float[Tensor, \"inst feats d_hidden\"]\n",
    "    b_final: Float[Tensor, \"inst feats\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        feature_probability: float | Tensor = 1.0,\n",
    "        importance: float | Tensor = 1.0,\n",
    "        device=device,\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if isinstance(feature_probability, float):\n",
    "            feature_probability = t.tensor(feature_probability)\n",
    "        self.feature_probability = feature_probability.to(device).broadcast_to(\n",
    "            (cfg.n_inst, cfg.n_features)\n",
    "        )\n",
    "        if isinstance(importance, float):\n",
    "            importance = t.tensor(importance)\n",
    "        self.importance = importance.to(device).broadcast_to((cfg.n_inst, cfg.n_features))\n",
    "\n",
    "        self.W1 = nn.Parameter(nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.d_hidden, cfg.n_features))))\n",
    "        self.W2 = nn.Parameter(nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.n_features, cfg.d_hidden))))\n",
    "        self.b_final = nn.Parameter(t.zeros((cfg.n_inst, cfg.n_features)))\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(self, features: Float[Tensor, \"... inst feats\"]) -> Float[Tensor, \"... inst feats\"]:\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        h1 = F.relu(einops.einsum(self.W1, features, \n",
    "                \"inst d_hidden feats,  ... inst feats -> ... inst d_hidden\"))\n",
    "        \n",
    "        out = F.relu(einops.einsum(self.W2, h1,\n",
    "                \"inst feats d_hidden, ... inst d_hidden -> ... inst feats\")\n",
    "                +self.b_final)\n",
    "        return out\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def generate_batch(self, batch_size) -> Tensor:\n",
    "        # YOUR CODE HERE\n",
    "        data_size = (batch_size, self.cfg.n_inst, self.cfg.n_features)\n",
    "        data = t.rand(data_size, device= self.W1.device)*2 - 1\n",
    "        pro = t.rand(data_size, device= self.W1.device)\n",
    "\n",
    "        return t.where(pro<self.feature_probability, data, 0.0)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        out: Float[Tensor, \"batch instances features\"],\n",
    "        batch: Float[Tensor, \"batch instances features\"],\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        # YOUR CODE HERE\n",
    "        error = self.importance * ( (out-batch.abs())**2 )\n",
    "        loss = einops.reduce(error,\n",
    "                             \"batch inst feats->inst\", \"mean\").sum()\n",
    "        return loss\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_neuron_computation_model(NeuronComputationModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(n_inst=7, n_features=100, d_hidden=40)\n",
    "\n",
    "importance = 0.8 ** t.arange(1, 1 + cfg.n_features)\n",
    "feature_probability = t.tensor([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001])\n",
    "\n",
    "model = NeuronComputationModel(\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    importance=importance[None, :],\n",
    "    feature_probability=feature_probability[:, None],\n",
    ")\n",
    "model.optimize(steps=10_000)\n",
    "\n",
    "utils.plot_features_in_Nd(\n",
    "    model.W1,\n",
    "    height=800,\n",
    "    width=1400,\n",
    "    neuron_plot=True,\n",
    "    subplot_titles=[f\"1 - S = {i:.3f}<br>\" for i in feature_probability.squeeze()],\n",
    "    title=f\"Neuron computation model: {cfg.n_features=}, {cfg.d_hidden=}, I<sub>i</subï¿½> = 0.75<sup>i</sup>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(n_inst=5, n_features=10, d_hidden=10)\n",
    "\n",
    "importance = 0.8 ** t.arange(1, 1 + cfg.n_features)\n",
    "feature_probability = 0.5\n",
    "\n",
    "model = NeuronComputationModel(\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    importance=importance[None, :],\n",
    "    feature_probability=feature_probability,\n",
    ")\n",
    "model.optimize(steps=10_000)\n",
    "\n",
    "utils.plot_features_in_Nd_discrete(\n",
    "    W1=model.W1,\n",
    "    W2=model.W2,\n",
    "    title=\"Neuron computation model (colored discretely, by feature)\",\n",
    "    legend_names=[\n",
    "        f\"I<sub>{i}</sub> = {importance.squeeze()[i]:.3f}\" for i in range(cfg.n_features)\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
