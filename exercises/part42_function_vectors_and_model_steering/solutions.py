# %%

import sys
import time
from collections import defaultdict
from pathlib import Path
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import circuitsvis as cv
import einops
import numpy as np
import torch as t
from IPython.display import display
from jaxtyping import Float
from nnsight import LanguageModel, CONFIG
from rich import print as rprint
from rich.table import Table
from torch import Tensor
CONFIG.set_default_api_key("7592caadcba94ba2a9e3e008a8a3f6a2")

# Make sure exercises are in the path
if str(exercises_dir := Path(__file__).parent.parent) not in sys.path:
    sys.path.append(str(exercises_dir))

import part42_function_vectors_and_model_steering.tests as tests
from plotly_utils import imshow

section_dir = exercises_dir / "part42_function_vectors_and_model_steering"

device = t.device(
    "mps" if t.backends.mps.is_available() else "cuda" if t.cuda.is_available() else "cpu"
)

t.set_grad_enabled(False)

N_LAYERS = 28
N_HEADS = 16
D_MODEL = 4096
D_HEAD = D_MODEL // N_HEADS

# root should be the "function_vectors" path
root = Path(__file__).parent

MAIN = __name__ == "__main__"

REMOTE = True

# %%

if MAIN:
    model = LanguageModel("EleutherAI/gpt-j-6b", device_map="auto")
    tokenizer = model.tokenizer
    prompt = "The Eiffel Tower is in the city of"

# %%

if MAIN:
    with model.trace(remote=REMOTE) as runner:
        with runner.invoke(prompt) as invoker:
            attn_patterns = model.transformer.h[0].attn.attn_dropout.input[0][0].save()

    # Get string tokens (replacing special character for spaces)
    str_tokens = model.tokenizer.tokenize(prompt)
    str_tokens = [s.replace("Ġ", " ") for s in str_tokens]

    # Attention patterns (squeeze out the batch dimension)
    attn_patterns_value = attn_patterns.value.squeeze(0)

    print("Layer 0 Head Attention Patterns:")
    display(
        cv.attention.attention_patterns(
            tokens=str_tokens,
            attention=attn_patterns_value,
        )
    )

# %%


class ICLSequence:
    """
    Class to store a single antonym sequence.

    Uses the default template "Q: {x}\nA: {y}" (with separate pairs split by "\n\n").
    """

    def __init__(self, word_pairs: list[list[str]]):
        self.word_pairs = word_pairs
        self.x, self.y = zip(*word_pairs)

    def __len__(self):
        return len(self.word_pairs)

    def __getitem__(self, idx: int):
        return self.word_pairs[idx]

    def prompt(self):
        """Returns the prompt, which contains all but the second element in the last word pair."""
        p = "\n\n".join([f"Q: {x}\nA: {y}" for x, y in self.word_pairs])
        return p[: -len(self.completion())]

    def completion(self):
        """Returns the second element in the last word pair (with padded space)."""
        return " " + self.y[-1]

    def __str__(self):
        """Prints a readable string representation of the prompt & completion (indep of template)."""
        return f"{', '.join([f'({x}, {y})' for x, y in self[:-1]])}, {self.x[-1]} ->".strip(", ")


# %%


class ICLDataset:
    """
    Dataset to create antonym pair prompts, in ICL task format. We use random seeds for consistency
    between the corrupted and clean datasets.

    Inputs:
        word_pairs:
            list of ICL task, e.g. [["old", "young"], ["top", "bottom"], ...] for the antonym task
        size:
            number of prompts to generate
        n_prepended:
            number of antonym pairs before the single-word ICL task
        bidirectional:
            if True, then we also consider the reversed antonym pairs
        corrupted:
            if True, then the second word in each pair is replaced with a random word
        seed:
            random seed, for consistency & reproducibility
    """

    def __init__(
        self,
        word_pairs: list[list[str]],
        size: int,
        n_prepended: int,
        bidirectional: bool = True,
        corrupted: bool = False,
        seed: int = 0,
    ):
        assert n_prepended + 1 <= len(
            word_pairs
        ), "Not enough antonym pairs in dataset to create prompt."

        self.word_pairs = word_pairs
        self.word_list = [word for word_pair in word_pairs for word in word_pair]
        self.size = size
        self.n_prepended = n_prepended
        self.bidirectional = bidirectional
        self.corrupted = corrupted
        self.seed = seed

        self.seqs = []
        self.prompts = []
        self.completions = []

        # Generate the dataset (by choosing random word pairs, and constructing `ICLSequence` objects)
        for n in range(size):
            np.random.seed(seed + n)
            random_pairs = np.random.choice(len(self.word_pairs), n_prepended + 1, replace=False)
            # Randomize the order of each word pair (x, y). If not bidirectional, we always have x -> y not y -> x
            random_orders = np.random.choice([1, -1], n_prepended + 1)
            if not (bidirectional):
                random_orders[:] = 1
            word_pairs = [
                self.word_pairs[pair][::order] for pair, order in zip(random_pairs, random_orders)
            ]
            # If corrupted, then replace y with a random word in all (x, y) pairs except the last one
            if corrupted:
                for i in range(len(word_pairs) - 1):
                    word_pairs[i][1] = np.random.choice(self.word_list)
            seq = ICLSequence(word_pairs)

            self.seqs.append(seq)
            self.prompts.append(seq.prompt())
            self.completions.append(seq.completion())

    def create_corrupted_dataset(self):
        """Creates a corrupted version of the dataset (with same random seed)."""
        return ICLDataset(
            self.word_pairs,
            self.size,
            self.n_prepended,
            self.bidirectional,
            corrupted=True,
            seed=self.seed,
        )

    def __len__(self):
        return self.size

    def __getitem__(self, idx: int):
        return self.seqs[idx]


# %%


def calculate_h(
    model: LanguageModel, dataset: ICLDataset, layer: int = -1
) -> tuple[list[str], Tensor]:
    """
    Averages over the model's hidden representations on each of the prompts in `dataset` at layer `layer`, to produce
    a single vector `h`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset whose prompts `dataset.prompts` you're extracting the activations from (at the last seq pos)
        layer: int
            the layer you're extracting activations from

    Returns:
        completions: list[str]
            list of model completion strings (i.e. the strings the model predicts to follow the last token)
        h: Tensor
            average hidden state tensor at final sequence position, of shape (d_model,)
    """
    with model.trace(remote=REMOTE) as runner:
        with runner.invoke(dataset.prompts) as invoker:
            hidden_states = model.transformer.h[layer].output[0][:, -1]
            h = hidden_states.mean(dim=0).save()

            logits = model.lm_head.output[:, -1]
            token_ids = logits.argmax(dim=-1).save()

    completions = model.tokenizer.batch_decode(token_ids.value)

    return completions, h.value


if MAIN:
    tests.test_calculate_h(calculate_h, model, solution=False)


# %%


def display_model_completions_on_antonyms(
    dataset: ICLDataset,
    completions: list[str],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Prompt (tuple representation)",
        "Model's completion\n(green=correct)",
        "Correct completion",
        title="Model's antonym completions",
    )

    for i in range(min(len(completions), num_to_display)):
        # Get model's completion, and correct completion
        completion = completions[i]
        correct_completion = dataset.completions[i]
        correct_completion_first_token = model.tokenizer.tokenize(correct_completion)[0].replace(
            "Ġ", " "
        )
        seq = dataset.seqs[i]

        # Color code the completion based on whether it's correct
        is_correct = completion == correct_completion_first_token
        completion = f"[b green]{repr(completion)}[/]" if is_correct else repr(completion)

        table.add_row(str(seq), completion, repr(correct_completion))

    rprint(table)


if MAIN:
    with open(root / "data" / "antonym_pairs.txt", "r") as f:
        ANTONYM_PAIRS = [line.split() for line in f.readlines()]

    # Get uncorrupted dataset
    dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=2)

    # Getting it from layer 12, as in the description in section 2.1 of paper
    layer = 12
    model_completions, h = calculate_h(model, dataset, layer=layer)

    # Displaying the output
    display_model_completions_on_antonyms(dataset, model_completions)


# %%


def intervene_with_h(
    model: LanguageModel,
    zero_shot_dataset: ICLDataset,
    h: Tensor,
    layer: int,
    remote: bool = True,
) -> tuple[list[str], list[str]]:
    """
    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the
    residual stream of a set of generated zero-shot prompts.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        h: Tensor
            the `h`-vector we'll be adding to the residual stream
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        completions_zero_shot: list[str]
            list of string completions for the zero-shot prompts, without intervention
        completions_intervention: list[str]
            list of string completions for the zero-shot prompts, with h-intervention
    """
    with model.trace(remote=remote) as runner:
        # First, run a forward pass where we don't intervene, just save token id completions
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            token_completions_zero_shot = model.lm_head.output[:, -1].argmax(dim=-1).save()

        # Next, run a forward pass on the zero-shot prompts where we do intervene
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            # Add the h-vector to the residual stream, at the last sequence position
            hidden_states = model.transformer.h[layer].output[0]
            hidden_states[:, -1] += h
            # Also save completions
            token_completions_intervention = model.lm_head.output[:, -1].argmax(dim=-1).save()

    # Decode to get the string tokens
    completions_zero_shot = model.tokenizer.batch_decode(token_completions_zero_shot.value)
    completions_intervention = model.tokenizer.batch_decode(token_completions_intervention.value)

    return completions_zero_shot, completions_intervention


if MAIN:
    tests.test_intervene_with_h(intervene_with_h, model, h, ANTONYM_PAIRS, REMOTE)


# %%


def display_model_completions_on_h_intervention(
    dataset: ICLDataset,
    completions: list[str],
    completions_intervention: list[str],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Prompt",
        "Model's completion\n(no intervention)",
        "Model's completion\n(intervention)",
        "Correct completion",
        title="Model's antonym completions",
    )

    for i in range(min(len(completions), num_to_display)):
        completion_ni = completions[i]
        completion_i = completions_intervention[i]
        correct_completion = dataset.completions[i]
        correct_completion_first_token = tokenizer.tokenize(correct_completion)[0].replace("Ġ", " ")
        seq = dataset.seqs[i]

        # Color code the completion based on whether it's correct
        is_correct = completion_i == correct_completion_first_token
        completion_i = f"[b green]{repr(completion_i)}[/]" if is_correct else repr(completion_i)

        table.add_row(str(seq), repr(completion_ni), completion_i, repr(correct_completion))

    rprint(table)


if MAIN:
    dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
    zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)
    completions_zero_shot, completions_intervention = intervene_with_h(
        model, zero_shot_dataset, h, layer=layer
    )
    display_model_completions_on_h_intervention(
        zero_shot_dataset, completions_zero_shot, completions_intervention
    )


# %%


def calculate_h_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> tuple[list[str], list[str]]:
    """
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the completions from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        completions_zero_shot: list[str]
            list of string completions for the zero-shot prompts, without intervention
        completions_intervention: list[str]
            list of string completions for the zero-shot prompts, with h-intervention
    """
    with model.trace(remote=REMOTE) as runner:
        # Run on the clean prompts, to get the h-vector
        with runner.invoke(dataset.prompts) as invoker:
            # Define h (we don't need to save it, cause we don't need it outside `runner:`)
            hidden_states = model.transformer.h[layer].output[0]
            h = hidden_states[:, -1].mean(dim=0)

        # First, run a forward pass where we don't intervene
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            token_completions_zero_shot = model.lm_head.output[:, -1].argmax(dim=-1).save()

        # Next, run a forward pass on the zero-shot prompts where we do intervene
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            # Add the h-vector to the residual stream, at the last sequence position
            hidden_states = model.transformer.h[layer].output[0]
            hidden_states[:, -1] += h
            # Also save completions
            token_completions_intervention = model.lm_head.output[:, -1].argmax(dim=-1).save()

    # Decode to get the string tokens
    completions_zero_shot = model.tokenizer.batch_decode(token_completions_zero_shot.value)
    completions_intervention = model.tokenizer.batch_decode(token_completions_intervention.value)

    return completions_zero_shot, completions_intervention


if MAIN:
    dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
    zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)
    completions_zero_shot, completions_intervention = calculate_h_and_intervene(
        model, dataset, zero_shot_dataset, layer=layer
    )
    display_model_completions_on_h_intervention(
        zero_shot_dataset, completions_zero_shot, completions_intervention
    )


# %%


def calculate_h_and_intervene_logprobs(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> tuple[list[float], list[float]]:
    """
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the logprobs on correct tokens from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        correct_logprobs: list[float]
            list of correct-token logprobs for the zero-shot prompts, without intervention
        correct_logprobs_intervention: list[float]
            list of correct-token logprobs for the zero-shot prompts, with h-intervention
    """
    # Get correct completions from `dataset`, to be used for indexing into the logprobs
    correct_completion_ids = [
        toks[0] for toks in tokenizer(zero_shot_dataset.completions)["input_ids"]
    ]

    with model.trace(remote=REMOTE) as runner:
        # Run on the clean prompts, to get the h-vector
        with runner.invoke(dataset.prompts) as invoker:
            # Define h (we don't need to save it, cause we don't need it outside `runner:`)
            hidden_states = model.transformer.h[layer].output[0]
            h = hidden_states[:, -1].mean(dim=0)

        # First, run a forward pass where we don't intervene
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            # We save correct-token logprobs, not all logits - this means less for us to download!
            logprobs = model.lm_head.output[:, -1].log_softmax(dim=-1)
            correct_logprobs_zero_shot = logprobs[
                t.arange(len(zero_shot_dataset)), correct_completion_ids
            ].save()

        # Next, run a forward pass on the zero-shot prompts where we do intervene
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            # Add the h-vector to the residual stream, at the last sequence position
            hidden_states = model.transformer.h[layer].output[0]
            hidden_states[:, -1] += h
            # We save correct-token logprobs, not all logits - this means less for us to download!
            logprobs = model.lm_head.output[:, -1].log_softmax(dim=-1)
            correct_logprobs_intervention = logprobs[
                t.arange(len(zero_shot_dataset)), correct_completion_ids
            ].save()

    return correct_logprobs_zero_shot.value.tolist(), correct_logprobs_intervention.value.tolist()


def display_model_logprobs_on_h_intervention(
    dataset: ICLDataset,
    correct_logprobs_zero_shot: list[float],
    correct_logprobs_intervention: list[float],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Zero-shot prompt",
        "Model's logprob\n(no intervention)",
        "Model's logprob\n(intervention)",
        "Change in logprob",
        title="Model's antonym logprobs, with zero-shot h-intervention\n(green = intervention improves accuracy)",
    )

    for i in range(min(len(correct_logprobs_zero_shot), num_to_display)):
        logprob_ni = correct_logprobs_zero_shot[i]
        logprob_i = correct_logprobs_intervention[i]
        delta_logprob = logprob_i - logprob_ni
        zero_shot_prompt = f"{dataset[i].x[0]:>8} -> {dataset[i].y[0]}"

        # Color code the logprob based on whether it's increased with this intervention
        is_improvement = delta_logprob >= 0
        delta_logprob = (
            f"[b green]{delta_logprob:+.2f}[/]" if is_improvement else f"{delta_logprob:+.2f}"
        )

        table.add_row(zero_shot_prompt, f"{logprob_ni:.2f}", f"{logprob_i:.2f}", delta_logprob)

    rprint(table)


if MAIN:
    dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
    zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)
    correct_logprobs_zero_shot, correct_logprobs_intervention = calculate_h_and_intervene_logprobs(
        model, dataset, zero_shot_dataset, layer=layer
    )
    display_model_logprobs_on_h_intervention(
        zero_shot_dataset, correct_logprobs_zero_shot, correct_logprobs_intervention
    )


# %%


def calculate_fn_vectors_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    layers: list[int] | None = None,
) -> Float[Tensor, "layers heads"]:
    """
    Returns a tensor of shape (layers, heads), containing the CIE for each head.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a
            corrupted version of this dataset for interventions)
        layers: list[int] | None
            the layers which this function will calculate the score for (if None, we assume all layers)
    """
    layers_list = list(range(model.config.n_layer)) if (layers is None) else layers
    heads = range(model.config.n_head)
    n_heads = len(layers_list) * len(heads)

    # Get corrupted dataset
    corrupted_dataset = dataset.create_corrupted_dataset()
    N = len(dataset)

    # Get correct token ids, so we can get correct token logprobs
    correct_completion_ids = [toks[0] for toks in tokenizer(dataset.completions)["input_ids"]]  # type: ignore

    with model.trace(remote=REMOTE) as runner:
        # Run a forward pass on clean prompts, where we store attention head outputs in `z_dict`, for later use
        z_dict = {}
        with runner.invoke(dataset.prompts) as invoker:
            for layer in layers_list:
                # Get hidden states, reshape to get head dimension, store the mean tensor
                z = model.transformer.h[layer].attn.out_proj.input[0][0][:, -1]
                z_reshaped = z.reshape(N, N_HEADS, D_HEAD).mean(dim=0)
                for head in heads:
                    z_dict[(layer, head)] = z_reshaped[head]

        # Run a forward pass on corrupted prompts, where we don't intervene or store activations (just so we can
        # get the correct-token logprobs to compare with our intervention)
        with runner.invoke(corrupted_dataset.prompts) as invoker:
            logits = model.lm_head.output[:, -1]
            correct_logprobs_corrupted = logits.log_softmax(dim=-1)[
                t.arange(N), correct_completion_ids
            ].save()

        # For each head, run a forward pass on corrupted prompts (here we need multiple different forward passes,
        # because we're doing different interventions each time)
        correct_logprobs_dict = {}
        for layer in layers:
            for head in heads:
                with runner.invoke(corrupted_dataset.prompts) as invoker:
                    # Get hidden states, reshape to get head dimension, then set it to the a-vector
                    z = model.transformer.h[layer].attn.out_proj.input[0][0][:, -1]
                    z.reshape(N, N_HEADS, D_HEAD)[:, head] = z_dict[(layer, head)]
                    # Get logprobs at the end, which we'll compare with our corrupted logprobs
                    logits = model.lm_head.output[:, -1]
                    correct_logprobs_dict[(layer, head)] = logits.log_softmax(dim=-1)[
                        t.arange(N), correct_completion_ids
                    ].save()

    # Get difference between intervention logprobs and corrupted logprobs, and take mean over batch dim
    all_correct_logprobs_intervention = einops.rearrange(
        t.stack([v.value for v in correct_logprobs_dict.values()]),
        "(layers heads) batch -> layers heads batch",
        layers=len(layers),
    )
    logprobs_diff = (
        all_correct_logprobs_intervention - correct_logprobs_corrupted.value
    )  # shape [layers heads batch]

    # Return mean effect of intervention, over the batch dimension
    return logprobs_diff.mean(dim=-1)


if MAIN:
    dataset = ICLDataset(ANTONYM_PAIRS, size=4, n_prepended=2)

    def batch_process_layers(n_layers, batch_size):
        for i in range(0, n_layers, batch_size):
            yield range(n_layers)[i : i + batch_size]

    results = t.empty((0, N_HEADS), device=device)

    # If this fails to run, reduce the batch size so the fwd passes are split up more
    for layers in batch_process_layers(N_LAYERS, batch_size=4):
        print(f"Computing layers in {layers} ...")
        t0 = time.time()
        results = t.concat(
            [results, calculate_fn_vectors_and_intervene(model, dataset, layers).to(device)]
        )
        print(f"... finished in {time.time()-t0:.2f} seconds.\n")

    imshow(
        results.T,
        title="Average indirect effect of function-vector intervention on antonym task",
        width=1000,
        height=600,
        labels={"x": "Layer", "y": "Head"},
        aspect="equal",
    )


# %%


def calculate_fn_vector(
    model: LanguageModel,
    dataset: ICLDataset,
    head_list: list[tuple[int, int]],
) -> Float[Tensor, "d_model"]:
    """
    Returns a vector of length `d_model`, containing the sum of vectors written to the residual stream
    by the attention heads in `head_list`, averaged over all inputs in `dataset`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a
            corrupted version of this dataset for interventions)
        head_list: list[tuple[int, int]]
            list of attention heads we're calculating the function vector from
    """
    # Turn head_list into a dict of {layer: heads we need in this layer}
    head_dict = defaultdict(set)
    for layer, head in head_list:
        head_dict[layer].add(head)

    fn_vector_list = []

    with model.trace(remote=REMOTE) as runner:
        with runner.invoke(dataset.prompts) as invoker:
            for layer, head_list in head_dict.items():
                # Get the output projection layer
                out_proj = model.transformer.h[layer].attn.out_proj

                # Get the mean output projection input (note, since this is the mean, so we don't need to
                # worry about changing the values of this tensor having downstream effects)
                hidden_states = out_proj.input[0][0][:, -1].mean(dim=0)
                assert hidden_states.shape == (D_MODEL,)

                # Zero-ablate all heads which aren't in our list, then get the output (which
                # will be the sum over the heads we actually do want!)
                heads_to_ablate = set(range(N_HEADS)) - head_dict[layer]
                for head in heads_to_ablate:
                    hidden_states.reshape(N_HEADS, D_HEAD)[head] = 0.0

                # Now that we've zeroed all unimportant heads, get the output & add it to the list
                # (we need a single batch dimension so we can use `out_proj`)
                out_proj_output = out_proj(hidden_states.unsqueeze(0)).squeeze()
                fn_vector_list.append(out_proj_output.save())

    # We sum all attention head outputs to get our function vector
    fn_vector = sum([v.value for v in fn_vector_list])

    assert fn_vector.shape == (D_MODEL,)
    return fn_vector


if MAIN:
    tests.test_calculate_fn_vector(calculate_fn_vector, model, solution=False)


# %%


def intervene_with_fn_vector(
    model: LanguageModel,
    word: str,
    layer: int,
    fn_vector: Float[Tensor, "d_model"],
    prompt_template='The word "{x}" means',
    n_tokens: int = 5,
) -> tuple[str, str]:
    """
    Intervenes with a function vector, by adding it at the last sequence position of a generated prompt.

    Inputs:
        word: str
            The word which is substituted into the prompt template, via prompt_template.format(x=word)
        layer: int
            The layer we'll make the intervention (by adding the function vector)
        fn_vector: Float[Tensor, "d_model"]
            The vector we'll add to the final sequence position for each new token to be generated
        prompt_template:
            The template of the prompt we'll use to produce completions
        n_tokens: int
            The number of additional tokens we'll generate for our unsteered / steered completions

    Returns:
        completion: str
            The full completion (including original prompt) for the no-intervention case
        completion_intervention: str
            The full completion (including original prompt) for the intervention case
    """
    prompt = prompt_template.format(x=word)

    # pad_token_id=tokenizer.eos_token_id
    with model.generate(max_new_tokens=n_tokens, remote=REMOTE) as generator:
        # No intervention
        with generator.invoke(prompt) as invoker:
            # We have to run `model.next()` until we're at the final token, then save its logits
            for i in range(n_tokens - 1):
                model.next()
            id = model.generator.output.save()

        # Intervention
        with generator.invoke(prompt) as invoker:
            for i in range(n_tokens):
                # Intervene with the function vector
                hidden_states = model.transformer.h[layer].output[0]
                hidden_states[:, -1] += fn_vector

                # Step the invoker
                model.next()
            id_int = model.generator.output.save()

    # generator is in scope because we used remote_include_output
    completion = tokenizer.batch_decode(id.value)
    completion_intervention = tokenizer.batch_decode(id_int.value)
    return completion, completion_intervention


if MAIN:
    # Remove "light" from our pairs, so it can be a holdout
    word = "light"
    _ANTONYM_PAIRS = [pair for pair in ANTONYM_PAIRS if word not in pair]

    # Define our dataset, and the attention heads we'll use
    dataset = ICLDataset(_ANTONYM_PAIRS, size=20, n_prepended=5)
    head_list = [
        (8, 0),
        (8, 1),
        (9, 14),
        (11, 0),
        (12, 10),
        (13, 12),
        (13, 13),
        (14, 9),
        (15, 5),
        (16, 14),
    ]

    # Extract the function vector
    fn_vector = calculate_fn_vector(model, dataset, head_list)

    # Intervene with the function vector
    completion, completion_intervention = intervene_with_fn_vector(
        model,
        word=word,
        layer=9,
        fn_vector=fn_vector,
        prompt_template='The word "{x}" means',
        n_tokens=40,
    )

    table = Table("No intervention", "intervention")
    table.add_row(repr(completion), repr(completion_intervention))
    rprint(table)


# %%


if MAIN:
    # I found using GPT4 directly was easier than using the API for this function. I took the output from GPT4
    # and copied it directly into a text file, then read it in:
    with open(section_dir / "data" / "country_capital_pairs.txt", "r", encoding="utf-8") as f:
        COUNTRY_CAPITAL_PAIRS = [line.split() for line in f.readlines()]

    # Remove (Netherlands, Amsterdam) from the pairs, so it can be a holdout
    country = "Netherlands"
    _COUNTRY_CAPITAL_PAIRS = [pair for pair in COUNTRY_CAPITAL_PAIRS if pair[0] != country]

    # Define our dataset, and the attention heads we'll use
    dataset = ICLDataset(_COUNTRY_CAPITAL_PAIRS, size=20, n_prepended=5, bidirectional=False)
    head_list = [
        (8, 0),
        (8, 1),
        (9, 14),
        (11, 0),
        (12, 10),
        (13, 12),
        (13, 13),
        (14, 9),
        (15, 5),
        (16, 14),
    ]

    # Extract the function vector
    fn_vector = calculate_fn_vector(model, dataset, head_list)

    # Intervene with the function vector
    completion, completion_intervention = intervene_with_fn_vector(
        model=model,
        word=country,
        layer=9,
        fn_vector=fn_vector,
        prompt_template="When you think of {x},",
        n_tokens=40,
    )

    table = Table("No intervention", "intervention")
    table.add_row(repr(completion), repr(completion_intervention))
    rprint(table)


# %%

if MAIN:
    model = LanguageModel("openai-community/gpt2-xl")
    tokenizer = model.tokenizer


sampling_kwargs = {
    "do_sample": True,
    "top_p": 0.3,
    "repetition_penalty": 1.1,
}


def calculate_and_apply_steering_vector(
    model: LanguageModel,
    prompt: str,
    activation_additions: list[tuple[int, float, str]],
    n_tokens: int,
    n_comparisons: int = 1,
    use_bos: bool = True,
) -> tuple[list[str], list[str]]:
    """
    Performs the steering vector experiments described in the LessWrong post.

    Args:
        prompt: str
            The original prompt, which we'll be doing activation steering on.

        activation_additions: list[tuple[int, float, str]], each tuple contains:
            layer - the layer we're applying these steering vectors to
            coefficient - the value we're multiplying it by
            prompt - the prompt we're inputting
            e.g. activation_additions[0] = [6, 5.0, " Love"] means we add the " Love" vector at layer 6, scaled by 5x

        n_tokens: int
            Number of tokens which will be generated for each completion

        n_comparisons: int
            Number of sequences generated in this function (i.e. we generate `n_comparisons` which are unsteered, and
            the same number which are steered).

    Returns:
        unsteered_completions: list[str]
            list of length `n_comparisons`, containing all the unsteered completions.

        steered_completions: list[str]
            list of length `n_comparisons`, containing all the steered completions.
    """
    # Add the BOS token manually, if we're including it
    if use_bos:
        bos = model.tokenizer.bos_token
        prompt = bos + prompt
        activation_additions = [[layer, coeff, bos + p] for layer, coeff, p in activation_additions]

    # Get the (layers, coeffs, prompts) in an easier form to use, also calculate the prompt lengths & check they're all the same
    act_add_layers, act_add_coeffs, act_add_prompts = zip(*activation_additions)
    act_add_seq_lens = [len(tokenizer.tokenize(p)) for p in act_add_prompts]
    assert (
        len(set(act_add_seq_lens)) == 1
    ), "All activation addition prompts must be the same length."
    assert act_add_seq_lens[0] <= len(
        tokenizer.tokenize(prompt)
    ), "All act_add prompts should be shorter than original prompt."

    # Get the prompts we'll intervene on (unsteered and steered)
    steered_prompts = [prompt for _ in range(n_comparisons)]
    unsteered_prompts = [prompt for _ in range(n_comparisons)]

    with model.generate(max_new_tokens=n_tokens, remote=True, **sampling_kwargs) as generator:
        # Run the act_add prompts (i.e. the contrast pairs), and extract their activations
        with generator.invoke(act_add_prompts) as invoker:
            # Get all the prompts from the activation additions, and put them in a list
            # (note, we slice from the end of the sequence because of left-padding)
            act_add_vectors = [
                model.transformer.h[layer].output[0][i, -seq_len:]
                for i, (layer, seq_len) in enumerate(zip(act_add_layers, act_add_seq_lens))
            ]

        # Forward pass on unsteered prompts (no intervention, no activations saved - we only need the completions)
        with generator.invoke(unsteered_prompts) as invoker:
            unsteered_out = model.generator.output.save()

        # Forward pass on steered prompts (we add in the results from the act_add prompts)
        with generator.invoke(steered_prompts) as invoker:
            # For each act_add prompt, add the vector to residual stream, at the start of the sequence
            for i, (layer, coeff, seq_len) in enumerate(
                zip(act_add_layers, act_add_coeffs, act_add_seq_lens)
            ):
                model.transformer.h[layer].output[0][:, :seq_len] += act_add_vectors[i] * coeff
            steered_out = model.generator.output.save()

    # Decode steered & unsteered completions (discarding the sequences we only used for extracting activations) & return results
    unsteered_completions = tokenizer.batch_decode(unsteered_out[-n_comparisons:])
    steered_completions = tokenizer.batch_decode(steered_out[-n_comparisons:])
    # unsteered_completions = tokenizer.batch_decode(model.generator.output[-2*n_comparisons: -n_comparisons])
    # steered_completions = tokenizer.batch_decode(model.generator.output[-n_comparisons:])
    return unsteered_completions, steered_completions


if MAIN:
    unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
        model,
        prompt="I hate you because",
        activation_additions=[
            (6, +5.0, "Love "),
            (6, -5.0, "Hate"),
        ],
        n_tokens=50,
        n_comparisons=3,
        use_bos=True,
    )

    table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
    for usc, sc in zip(unsteered_completions, steered_completions):
        table.add_row(usc, sc)
    rprint(table)


if MAIN:
    unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
        model,
        prompt="I went up to my friend and said",
        activation_additions=[
            (20, +4.0, "I talk about weddings constantly  "),
            (20, -4.0, "I do not talk about weddings constantly"),
        ],
        n_tokens=50,
        n_comparisons=3,
        use_bos=False,
    )

    table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
    for usc, sc in zip(unsteered_completions, steered_completions):
        table.add_row(usc, sc)
    rprint(table)


if MAIN:
    unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
        model,
        prompt="To see the eiffel tower, people flock to",
        activation_additions=[
            (24, +10.0, "The Eiffel Tower is in Rome"),
            (24, -10.0, "The Eiffel Tower is in France"),
        ],
        n_tokens=50,
        n_comparisons=3,
        use_bos=False,
    )

    table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
    for usc, sc in zip(unsteered_completions, steered_completions):
        table.add_row(usc, sc)
    rprint(table)
# %%
