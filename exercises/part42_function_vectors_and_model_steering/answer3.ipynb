{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from IPython.display import display\n",
    "from jaxtyping import Float\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from torch import Tensor\n",
    "import string as s\n",
    "\n",
    "# Hide bunch of info logging messages from nnsight\n",
    "import logging, warnings\n",
    "logging.disable(sys.maxsize)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='huggingface_hub.utils._token')\n",
    "\n",
    "device = t.device('mps' if t.backends.mps.is_available() else 'cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformer_interp\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part42_function_vectors_and_model_steering\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from plotly_utils import imshow\n",
    "import part42_function_vectors_and_model_steering.solutions as solutions\n",
    "import part42_function_vectors_and_model_steering.tests as tests\n",
    "\n",
    "MAIN = __name__ == '__main__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel('EleutherAI/gpt-j-6b', device_map='auto', torch_dtype=t.bfloat16)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "N_HEADS = model.config.n_head\n",
    "N_LAYERS = model.config.n_layer\n",
    "D_MODEL = model.config.n_embd\n",
    "D_HEAD = D_MODEL // N_HEADS\n",
    "\n",
    "print(f\"Number of heads: {N_HEADS}\")\n",
    "print(f\"Number of layers: {N_LAYERS}\")\n",
    "print(f\"Model dimension: {D_MODEL}\")\n",
    "print(f\"Head dimension: {D_HEAD}\\n\")\n",
    "\n",
    "print(\"Entire config: \", model.config)\n",
    "\n",
    "REMOTE = True\n",
    "# If you want to set REMOTE = True then you'll need an API key. Please join the NDIF community\n",
    "# Discord (https://nnsight.net/status/) and request one from there, then uncomment and run the\n",
    "# following code:\n",
    "CONFIG.set_default_api_key(\"7592caadcba94ba2a9e3e008a8a3f6a2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace(\"Hello,\", remote=REMOTE):\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word pairs from the text file\n",
    "with open(section_dir / \"data\" / \"antonym_pairs.txt\", \"r\") as f:\n",
    "    ANTONYM_PAIRS = [line.split() for line in f.readlines()]\n",
    "\n",
    "print(ANTONYM_PAIRS[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLSequence:\n",
    "    '''\n",
    "    Class to store a single antonym sequence.\n",
    "\n",
    "    Uses the default template \"Q: {x}\\nA: {y}\" (with separate pairs split by \"\\n\\n\").\n",
    "    '''\n",
    "    def __init__(self, word_pairs: list[tuple[str, str]]):\n",
    "        self.word_pairs = word_pairs\n",
    "        self.x, self.y = zip(*word_pairs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.word_pairs[idx]\n",
    "\n",
    "    def prompt(self):\n",
    "        '''Returns the prompt, which contains all but the second element in the last word pair.'''\n",
    "        p = \"\\n\\n\".join([f\"Q: {x}\\nA: {y}\" for x, y in self.word_pairs])\n",
    "        return p[:-len(self.completion())]\n",
    "\n",
    "    def completion(self):\n",
    "        '''Returns the second element in the last word pair (with padded space).'''\n",
    "        return \" \" + self.y[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        '''Prints a readable string representation of the prompt & completion (indep of template).'''\n",
    "        return f\"{', '.join([f'({x}, {y})' for x, y in self[:-1]])}, {self.x[-1]} ->\".strip(\", \")\n",
    "\n",
    "\n",
    "class ICLDataset:\n",
    "    '''\n",
    "    Dataset to create antonym pair prompts, in ICL task format. We use random seeds for consistency\n",
    "    between the corrupted and clean datasets.\n",
    "\n",
    "    Inputs:\n",
    "        word_pairs:\n",
    "            list of ICL task, e.g. [[\"old\", \"young\"], [\"top\", \"bottom\"], ...] for the antonym task\n",
    "        size:\n",
    "            number of prompts to generate\n",
    "        n_prepended:\n",
    "            number of antonym pairs before the single-word ICL task\n",
    "        bidirectional:\n",
    "            if True, then we also consider the reversed antonym pairs\n",
    "        corrupted:\n",
    "            if True, then the second word in each pair is replaced with a random word\n",
    "        seed:\n",
    "            random seed, for consistency & reproducibility\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        word_pairs: list[tuple[str, str]],\n",
    "        size: int,\n",
    "        n_prepended: int,\n",
    "        bidirectional: bool = True,\n",
    "        seed: int = 0,\n",
    "        corrupted: bool = False,\n",
    "    ):\n",
    "        assert n_prepended+1 <= len(word_pairs), \"Not enough antonym pairs in dataset to create prompt.\"\n",
    "\n",
    "        self.word_pairs = word_pairs\n",
    "        self.word_list = [word for word_pair in word_pairs for word in word_pair]\n",
    "        self.size = size\n",
    "        self.n_prepended = n_prepended\n",
    "        self.bidirectional = bidirectional\n",
    "        self.corrupted = corrupted\n",
    "        self.seed = seed\n",
    "\n",
    "        self.seqs = []\n",
    "        self.prompts = []\n",
    "        self.completions = []\n",
    "\n",
    "        # Generate the dataset (by choosing random antonym pairs, and constructing `ICLSequence` objects)\n",
    "        for n in range(size):\n",
    "            np.random.seed(seed + n)\n",
    "            random_pairs = np.random.choice(len(self.word_pairs), n_prepended+1, replace=False)\n",
    "            random_orders = np.random.choice([1, -1], n_prepended+1)\n",
    "            if not(bidirectional): random_orders[:] = 1\n",
    "            word_pairs = [self.word_pairs[pair][::order] for pair, order in zip(random_pairs, random_orders)]\n",
    "            if corrupted:\n",
    "                for i in range(len(word_pairs) - 1):\n",
    "                    word_pairs[i][1] = np.random.choice(self.word_list)\n",
    "            seq = ICLSequence(word_pairs)\n",
    "\n",
    "            self.seqs.append(seq)\n",
    "            self.prompts.append(seq.prompt())\n",
    "            self.completions.append(seq.completion())\n",
    "\n",
    "    def create_corrupted_dataset(self):\n",
    "        '''Creates a corrupted version of the dataset (with same random seed).'''\n",
    "        return ICLDataset(self.word_pairs, self.size, self.n_prepended, self.bidirectional, corrupted=True, seed=self.seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.seqs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fn_vectors_and_intervene(\n",
    "    model: LanguageModel,\n",
    "    dataset: ICLDataset,\n",
    "    layers: list[int] | None = None,\n",
    ") -> Float[Tensor, \"layers heads\"]:\n",
    "    '''\n",
    "    Returns a tensor of shape (layers, heads), containing the CIE for each head.\n",
    "\n",
    "    Inputs:\n",
    "        model: LanguageModel\n",
    "            the transformer you're doing this computation with\n",
    "        dataset: ICLDataset\n",
    "            the dataset of clean prompts from which we'll extract the function vector (we'll also create a\n",
    "            corrupted version of this dataset for interventions)\n",
    "        layers: list[int] | None\n",
    "            the layers which this function will calculate the score for (if None, we assume all layers)\n",
    "    '''\n",
    "    if layers is None:\n",
    "        layer_list = list(range(model.config.n_layer))\n",
    "    else:\n",
    "        layer_list = layers\n",
    "    target_index = [tok[0] for tok in model.tokenizer(dataset.completions)[\"input_ids\"]]\n",
    "    b = len(dataset)\n",
    "    corrupted_dataset = dataset.create_corrupted_dataset()\n",
    "\n",
    "    with model.trace(remote=REMOTE) as tracer:\n",
    "        z_dict = {}\n",
    "        intervene_prob_dict = {}\n",
    "        with tracer.invoke(dataset.prompts):\n",
    "            for layer in layer_list:\n",
    "                z = model.transformer.h[layer].attn.out_proj.input[:,-1,:]\n",
    "                z_reshaped = z.reshape(b, N_HEADS, D_HEAD).mean(dim = 0)\n",
    "                for head in range(N_HEADS):\n",
    "                    z_dict[(layer, head)] = z_reshaped[head,:]\n",
    "\n",
    "        with tracer.invoke(corrupted_dataset.prompts):\n",
    "            logits = model.lm_head.output[:,-1,:]\n",
    "            clean_prob = logits.softmax(dim = -1)[t.arange(b), target_index].save()\n",
    "\n",
    "        for layer in layer_list:\n",
    "            for head in range(N_HEADS):\n",
    "                with tracer.invoke(corrupted_dataset.prompts):\n",
    "                    z = model.transformer.h[layer].attn.out_proj.input[:,-1,:]\n",
    "                    z.reshape(b,N_HEADS, D_HEAD)[:,head,:] = z_dict[(layer, head)]\n",
    "\n",
    "                    logits = model.lm_head.output[:,-1,:]\n",
    "                    intervene_prob_dict[(layer, head)] = logits.softmax(dim = -1)[t.arange(b), target_index].save()\n",
    "\n",
    "    intervene_prob_matrix = t.stack([v.value  for v in intervene_prob_dict.values()]).reshape(len(layers), N_HEADS, b)\n",
    "    diff = (intervene_prob_matrix - clean_prob.value).mean(dim = -1)\n",
    "    return diff\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ICLDataset(ANTONYM_PAIRS, size=8, n_prepended=2)\n",
    "\n",
    "def batch_process_layers(n_layers, batch_size):\n",
    "    for i in range(0, n_layers, batch_size):\n",
    "        yield range(n_layers)[i:i + batch_size]\n",
    "\n",
    "results = t.empty((0, N_HEADS), device=device)\n",
    "\n",
    "# If this fails to run, reduce the batch size so the fwd passes are split up more\n",
    "for layers in batch_process_layers(N_LAYERS, batch_size=4):\n",
    "\n",
    "    if layers[0] == 12:\n",
    "            break\n",
    "\n",
    "    print(f\"Computing layers in {layers} ...\")\n",
    "    t0 = time.time()\n",
    "    results = t.concat([results, calculate_fn_vectors_and_intervene(model, dataset, layers).to(device)])\n",
    "    print(f\"... finished in {time.time()-t0:.2f} seconds.\\n\")\n",
    "\n",
    "\n",
    "imshow(\n",
    "    results.T,\n",
    "    title = \"Average indirect effect of function-vector intervention on antonym task\",\n",
    "    width = 1000,\n",
    "    height = 600,\n",
    "    labels = {\"x\": \"Layer\", \"y\": \"Head\"},\n",
    "    aspect = \"equal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
