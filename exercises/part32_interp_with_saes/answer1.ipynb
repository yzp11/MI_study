{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Make sure exercises are in the path\n",
    "chapter = \"exercises\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}\").resolve()\n",
    "section_dir = (exercises_dir / \"part32_interp_with_saes\").resolve()\n",
    "if str(exercises_dir) not in sys.path:\n",
    "    sys.path.append(str(exercises_dir))\n",
    "\n",
    "import part31_superposition_and_saes.tests as part31_tests\n",
    "import part31_superposition_and_saes.utils as part31_utils\n",
    "import part32_interp_with_saes.tests as part32_tests\n",
    "import part32_interp_with_saes.utils as part32_utils\n",
    "from plotly_utils import imshow, line\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_pretrained_saes_directory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────────────────────────┬─────────────────────────────────────────────────────┬────────────────────────────────────────────────────────┬──────────┐\n",
      "│ model                               │ release                                             │ repo_id                                                │   n_saes │\n",
      "├─────────────────────────────────────┼─────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┼──────────┤\n",
      "│ gemma-2-27b                         │ gemma-scope-27b-pt-res                              │ google/gemma-scope-27b-pt-res                          │       18 │\n",
      "│ gemma-2-27b                         │ gemma-scope-27b-pt-res-canonical                    │ google/gemma-scope-27b-pt-res                          │        3 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-res                               │ google/gemma-scope-2b-pt-res                           │      310 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-res-canonical                     │ google/gemma-scope-2b-pt-res                           │       58 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-mlp                               │ google/gemma-scope-2b-pt-mlp                           │      260 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-mlp-canonical                     │ google/gemma-scope-2b-pt-mlp                           │       52 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-att                               │ google/gemma-scope-2b-pt-att                           │      260 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-att-canonical                     │ google/gemma-scope-2b-pt-att                           │       52 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_0824 │ canrager/lm_sae                                        │      180 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_0824 │ canrager/lm_sae                                        │      240 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824     │ canrager/lm_sae                                        │      180 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824     │ canrager/lm_sae                                        │      240 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-res                               │ google/gemma-scope-9b-pt-res                           │      562 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-res-canonical                     │ google/gemma-scope-9b-pt-res                           │       91 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-att                               │ google/gemma-scope-9b-pt-att                           │      492 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-att-canonical                     │ google/gemma-scope-9b-pt-att                           │       84 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-mlp                               │ google/gemma-scope-9b-pt-mlp                           │      492 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-mlp-canonical                     │ google/gemma-scope-9b-pt-mlp                           │       84 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-it-res                               │ google/gemma-scope-9b-it-res                           │       30 │\n",
      "│ gemma-2-9b-it                       │ gemma-scope-9b-it-res-canonical                     │ google/gemma-scope-9b-it-res                           │        6 │\n",
      "│ gemma-2b                            │ gemma-2b-res-jb                                     │ jbloom/Gemma-2b-Residual-Stream-SAEs                   │        5 │\n",
      "│ gemma-2b-it                         │ gemma-2b-it-res-jb                                  │ jbloom/Gemma-2b-IT-Residual-Stream-SAEs                │        1 │\n",
      "│ gpt2-small                          │ gpt2-small-res-jb                                   │ jbloom/GPT2-Small-SAEs-Reformatted                     │       13 │\n",
      "│ gpt2-small                          │ gpt2-small-hook-z-kk                                │ ckkissane/attn-saes-gpt2-small-all-layers              │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-mlp-tm                                   │ tommmcgrath/gpt2-small-mlp-out-saes                    │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-res-jb-feature-splitting                 │ jbloom/GPT2-Small-Feature-Splitting-Experiment-Layer-8 │        8 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-post-v5-32k                        │ jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs           │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-post-v5-128k                       │ jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs          │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-mid-v5-32k                         │ jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs            │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-mid-v5-128k                        │ jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs           │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-mlp-out-v5-32k                           │ jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs              │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-mlp-out-v5-128k                          │ jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs             │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-attn-out-v5-32k                          │ jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs             │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-attn-out-v5-128k                         │ jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs            │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-res_sll-ajt                              │ neuronpedia/gpt2-small__res_sll-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_slefr-ajt                            │ neuronpedia/gpt2-small__res_slefr-ajt                  │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_scl-ajt                              │ neuronpedia/gpt2-small__res_scl-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_sle-ajt                              │ neuronpedia/gpt2-small__res_sle-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_sce-ajt                              │ neuronpedia/gpt2-small__res_sce-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_scefr-ajt                            │ neuronpedia/gpt2-small__res_scefr-ajt                  │        3 │\n",
      "│ meta-llama/Meta-Llama-3-8B-Instruct │ llama-3-8b-it-res-jh                                │ Juliushanhanhan/llama-3-8b-it-res                      │        1 │\n",
      "│ mistral-7b                          │ mistral-7b-res-wg                                   │ JoshEngels/Mistral-7B-Residual-Stream-SAEs             │        3 │\n",
      "│ pythia-70m-deduped                  │ pythia-70m-deduped-res-sm                           │ ctigges/pythia-70m-deduped__res-sm_processed           │        7 │\n",
      "│ pythia-70m-deduped                  │ pythia-70m-deduped-mlp-sm                           │ ctigges/pythia-70m-deduped__mlp-sm_processed           │        6 │\n",
      "│ pythia-70m-deduped                  │ pythia-70m-deduped-att-sm                           │ ctigges/pythia-70m-deduped__att-sm_processed           │        6 │\n",
      "│ pythia-70m-deduped                  │ sae_bench_pythia70m_sweep_gated_ctx128_0730         │ canrager/lm_sae                                        │       40 │\n",
      "│ pythia-70m-deduped                  │ sae_bench_pythia70m_sweep_panneal_ctx128_0730       │ canrager/lm_sae                                        │       56 │\n",
      "│ pythia-70m-deduped                  │ sae_bench_pythia70m_sweep_standard_ctx128_0712      │ canrager/lm_sae                                        │       44 │\n",
      "│ pythia-70m-deduped                  │ sae_bench_pythia70m_sweep_topk_ctx128_0730          │ canrager/lm_sae                                        │       48 │\n",
      "└─────────────────────────────────────┴─────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "metadata_rows = [\n",
    "    [data.model, data.release, data.repo_id, len(data.saes_map)] for data in get_pretrained_saes_directory().values()\n",
    "]\n",
    "\n",
    "# Print all SAE releases, sorted by base model\n",
    "print(\n",
    "    tabulate(\n",
    "        sorted(metadata_rows, key=lambda x: x[0]),\n",
    "        headers=[\"model\", \"release\", \"repo_id\", \"n_saes\"],\n",
    "        tablefmt=\"simple_outline\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_value(value):\n",
    "    return \"{{{0!r}: {1!r}, ...}}\".format(*next(iter(value.items()))) if isinstance(value, dict) else repr(value)\n",
    "\n",
    "\n",
    "release = get_pretrained_saes_directory()[\"gpt2-small-res-jb\"]\n",
    "\n",
    "print(\n",
    "    tabulate(\n",
    "        [[k, format_value(v)] for k, v in release.__dict__.items()],\n",
    "        headers=[\"Field\", \"Value\"],\n",
    "        tablefmt=\"simple_outline\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[id, path, release.neuronpedia_id[id]] for id, path in release.saes_map.items()]\n",
    "\n",
    "print(\n",
    "    tabulate(\n",
    "        data,\n",
    "        headers=[\"SAE id\", \"SAE path (HuggingFace)\", \"Neuronpedia ID\"],\n",
    "        tablefmt=\"simple_outline\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/arena-env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/arena-env/lib/python3.11/site-packages/sae_lens/sae.py:145: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t.set_grad_enabled(False)\n",
    "\n",
    "gpt2: HookedSAETransformer = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "gpt2_sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    device=str(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.7.hook_resid_pre\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_sae.cfg.hook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(gpt2_sae.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dashboard(\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    latent_idx=0,\n",
    "    width=800,\n",
    "    height=600,\n",
    "):\n",
    "    release = get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = f\"https://neuronpedia.org/{neuronpedia_id}/{latent_idx}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "    print(url)\n",
    "    display(IFrame(url, width=width, height=height))\n",
    "\n",
    "\n",
    "latent_idx = random.randint(0, gpt2_sae.cfg.d_sae)\n",
    "latent_idx = 13\n",
    "display_dashboard(latent_idx=latent_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "utils.test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2.saes(saes=[gpt2_sae]):\n",
    "    utils.test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Same thing, done in a different way\n",
    "gpt2.add_sae(gpt2_sae)\n",
    "utils.test_prompt(prompt, answer, gpt2)\n",
    "gpt2.reset_saes()  # Remember to always do this!\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2.run_with_saes(prompt, saes=[gpt2_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.hook_resid_pre                    : (1, 13, 768)\n",
      "blocks.1.hook_resid_pre                    : (1, 13, 768)\n",
      "blocks.2.hook_resid_pre                    : (1, 13, 768)\n",
      "blocks.3.hook_resid_pre                    : (1, 13, 768)\n",
      "blocks.4.hook_resid_pre                    : (1, 13, 768)\n",
      "blocks.5.hook_resid_pre                    : (1, 13, 768)\n",
      "blocks.6.hook_resid_pre                    : (1, 13, 768)\n",
      "blocks.7.hook_resid_pre.hook_sae_input     : (1, 13, 768)\n",
      "blocks.7.hook_resid_pre.hook_sae_acts_pre  : (1, 13, 24576)\n",
      "blocks.7.hook_resid_pre.hook_sae_acts_post : (1, 13, 24576)\n",
      "blocks.7.hook_resid_pre.hook_sae_recons    : (1, 13, 768)\n",
      "blocks.7.hook_resid_pre.hook_sae_output    : (1, 13, 768)\n"
     ]
    }
   ],
   "source": [
    "_, cache = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae], stop_at_layer = gpt2_sae.cfg.hook_layer+1)\n",
    "\n",
    "for name, param in cache.items():\n",
    "    if 'resid_pre' in name:\n",
    "        print(f\"{name:<43}: {tuple(param.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_hook(\n",
    "        latent,\n",
    "        hook,\n",
    "):\n",
    "    print(latent.shape)\n",
    "    return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 24576])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logits = gpt2.run_with_hooks_with_saes(prompt, saes=[gpt2_sae],\n",
    "                                       fwd_hooks=[('blocks.7.hook_resid_pre.hook_sae_acts_post',latent_hook)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top activations on final token\n",
    "_, cache = gpt2.run_with_cache_with_saes(\n",
    "    prompt,\n",
    "    saes=[gpt2_sae],\n",
    "    stop_at_layer=gpt2_sae.cfg.hook_layer + 1,\n",
    ")\n",
    "sae_acts_post = cache[f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\"][0, -1, :]\n",
    "\n",
    "# Plot line chart of latent activations\n",
    "px.line(\n",
    "    sae_acts_post.cpu().numpy(),\n",
    "    title=f\"Latent activations at the final token position ({sae_acts_post.nonzero().numel()} alive)\",\n",
    "    labels={\"index\": \"Latent\", \"value\": \"Activation\"},\n",
    "    width=1000,\n",
    ").update_layout(showlegend=False).show()\n",
    "\n",
    "# Print the top 5 latents, and inspect their dashboards\n",
    "for act, ind in zip(*sae_acts_post.topk(3)):\n",
    "    print(f\"Latent {ind} had activation {act:.2f}\")\n",
    "    display_dashboard(latent_idx=ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_no_saes, cache_no_saes = gpt2.run_with_cache(prompt)\n",
    "\n",
    "gpt2_sae.use_error_term = False\n",
    "logits_with_sae_recon, cache_with_sae_recon = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae])\n",
    "\n",
    "gpt2_sae.use_error_term = True\n",
    "logits_without_sae_recon, cache_without_sae_recon = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae])\n",
    "\n",
    "# Both SAE caches contain the hook values\n",
    "assert f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\" in cache_with_sae_recon\n",
    "assert f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\" in cache_without_sae_recon\n",
    "\n",
    "# But the final output will be different, because we don't use SAE reconstructions when use_error_term=True\n",
    "t.testing.assert_close(logits_no_saes, logits_without_sae_recon)\n",
    "logit_diff_from_sae = (logits_no_saes - logits_with_sae_recon).abs().mean()\n",
    "print(f\"Average logit diff from using SAE reconstruction: {logit_diff_from_sae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt2_sae.cfg.dataset_path)\n",
    "print(gpt2_sae.cfg.dataset_trust_remote_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_act_store = ActivationsStore.from_sae(\n",
    "    model=gpt2,\n",
    "    sae=gpt2_sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Example of how you can use this:\n",
    "tokens = gpt2_act_store.get_batch_tokens()\n",
    "assert tokens.shape == (gpt2_act_store.store_batch_size_prompts, gpt2_act_store.context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dashboard(latent_idx=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_activation_histogram(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Displays the activation histogram for a particular latent, computed across `total_batches` batches from `act_store`.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "    all_positive_acts = []\n",
    "\n",
    "    for i in tqdm(range(total_batches)):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_post_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "        all_positive_acts.extend(acts[acts > 0].cpu().tolist())\n",
    "        \n",
    "        \n",
    "\n",
    "    frac_active = len(all_positive_acts) / (total_batches * act_store.store_batch_size_prompts * act_store.context_size)\n",
    "\n",
    "    px.histogram(\n",
    "        all_positive_acts,\n",
    "        nbins=50,\n",
    "        title=f\"ACTIVATIONS DENSITY {frac_active:.3%}\",\n",
    "        labels={\"value\": \"Activation\"},\n",
    "        width=800,\n",
    "        template=\"ggplot2\",\n",
    "        color_discrete_sequence=[\"darkorange\"],\n",
    "    ).update_layout(bargap=0.02, showlegend=False).show()\n",
    "\n",
    "\n",
    "show_activation_histogram(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_largest_indices(x: Float[Tensor, \"batch seq\"], k: int, buffer: int = 0) -> Int[Tensor, \"k 2\"]:\n",
    "    \"\"\"\n",
    "    The indices of the top k elements in the input tensor, i.e. output[i, :] is the (batch, seqpos) value of the i-th\n",
    "    largest element in x.\n",
    "\n",
    "    Won't choose any elements within `buffer` from the start or end of their sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    if buffer > 0:\n",
    "        x = x[:, buffer:-buffer]\n",
    "\n",
    "    indices = x.flatten().topk(k=k).indices\n",
    "\n",
    "    rows = indices // x.size(1)\n",
    "    cols = indices % x.size(1) + buffer\n",
    "\n",
    "    return t.stack((rows, cols), dim=1)\n",
    "\n",
    "\n",
    "x = t.arange(40, device=device).reshape((2, 20))\n",
    "x[0, 10] += 50  # 2nd highest value\n",
    "x[0, 11] += 100  # highest value\n",
    "x[1, 1] += 150  # not inside buffer (it's less than 3 from the start of the sequence)\n",
    "top_indices = get_k_largest_indices(x, k=2, buffer=3)\n",
    "assert top_indices.tolist() == [[0, 11], [0, 10]]\n",
    "\n",
    "\n",
    "def index_with_buffer(\n",
    "    x: Float[Tensor, \"batch seq\"], indices: Int[Tensor, \"k 2\"], buffer: int | None = None\n",
    ") -> Float[Tensor, \"k *buffer_x2_plus1\"]:\n",
    "    \"\"\"\n",
    "    Indexes into `x` with `indices` (which should have come from the `get_k_largest_indices` function), and takes a\n",
    "    +-buffer range around each indexed element. If `indices` are less than `buffer` away from the start of a sequence\n",
    "    then we just take the first `2*buffer+1` elems (same for at the end of a sequence).\n",
    "\n",
    "    If `buffer` is None, then we don't add any buffer and just return the elements at the given indices.\n",
    "    \"\"\"\n",
    "    rows, cols = indices.unbind(dim=-1)\n",
    "    if buffer is not None:\n",
    "        rows = einops.repeat(rows, \"k -> k buffer\", buffer=buffer * 2 + 1)\n",
    "        cols[cols < buffer] = buffer\n",
    "        cols[cols > x.size(1) - buffer - 1] = x.size(1) - buffer - 1\n",
    "        cols = einops.repeat(cols, \"k -> k buffer\", buffer=buffer * 2 + 1) + t.arange(\n",
    "            -buffer, buffer + 1, device=cols.device\n",
    "        )\n",
    "    return x[rows, cols]\n",
    "\n",
    "\n",
    "x_top_values_with_context = index_with_buffer(x, top_indices, buffer=3)\n",
    "assert x_top_values_with_context[0].tolist() == [8, 9, 10 + 50, 11 + 100, 12, 13, 14]  # highest value in the middle\n",
    "assert x_top_values_with_context[1].tolist() == [7, 8, 9, 10 + 50, 11 + 100, 12, 13]  # 2nd highest value in the middle\n",
    "\n",
    "\n",
    "def display_top_seqs(data: list[tuple[float, list[str], int]]):\n",
    "    \"\"\"\n",
    "    Given a list of (activation: float, str_toks: list[str], seq_pos: int), displays a table of these sequences, with\n",
    "    the relevant token highlighted.\n",
    "\n",
    "    We also turn newlines into \"\\\\n\", and remove unknown tokens � (usually weird quotation marks) for readability.\n",
    "    \"\"\"\n",
    "    table = Table(\"Act\", \"Sequence\", title=\"Max Activating Examples\", show_lines=True)\n",
    "    for act, str_toks, seq_pos in data:\n",
    "        formatted_seq = (\n",
    "            \"\".join([f\"[b u green]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)])\n",
    "            .replace(\"�\", \"\")\n",
    "            .replace(\"\\n\", \"↵\")\n",
    "        )\n",
    "        table.add_row(f\"{act:.3f}\", repr(formatted_seq))\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "example_data = [\n",
    "    (0.5, [\" one\", \" two\", \" three\"], 0),\n",
    "    (1.5, [\" one\", \" two\", \" three\"], 1),\n",
    "    (2.5, [\" one\", \" two\", \" three\"], 2),\n",
    "]\n",
    "display_top_seqs(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_max_activating_examples(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    "    display: bool = False,\n",
    ") -> list[tuple[float, list[str], int]]:\n",
    "    \"\"\"\n",
    "    Displays the max activating examples across a number of batches from the\n",
    "    activations store, using the `display_top_seqs` function.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "    # Create list to store the top k activations for each batch. Once we're done,\n",
    "    # we'll filter this to only contain the top k over all batches\n",
    "    data = []\n",
    "\n",
    "    for _ in tqdm(range(total_batches)):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_post_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "\n",
    "        # Get largest indices, get the corresponding max acts, and get the surrounding indices\n",
    "        k_largest_indices = get_k_largest_indices(acts, k=k, buffer=buffer)\n",
    "        tokens_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
    "        str_toks = [model.to_str_tokens(toks) for toks in tokens_with_buffer]\n",
    "        top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
    "        data.extend(list(zip(top_acts, str_toks, [buffer] * len(str_toks))))\n",
    "\n",
    "    data = sorted(data, key=lambda x: x[0], reverse=True)[:k]\n",
    "    if display:\n",
    "        display_top_seqs(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Display your results, and also test them\n",
    "buffer = 10\n",
    "data = fetch_max_activating_examples(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9, buffer=buffer, k=5, display=True)\n",
    "first_seq_str_tokens = data[0][1]\n",
    "assert first_seq_str_tokens[buffer] == \" new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_max_activating_examples(gpt2, gpt2_sae, gpt2_act_store, latent_idx=16873, total_batches=200, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_largest_indices(\n",
    "    x: Float[Tensor, \"batch seq\"],\n",
    "    k: int,\n",
    "    buffer: int = 0,\n",
    "    no_overlap: bool = True,\n",
    ") -> Int[Tensor, \"k 2\"]:\n",
    "    \"\"\"\n",
    "    Returns the tensor of (batch, seqpos) indices for each of the top k elements in the tensor x.\n",
    "\n",
    "    Args:\n",
    "        buffer:     We won't choose any elements within `buffer` from the start or end of their seq (this helps if we\n",
    "                    want more context around the chosen tokens).\n",
    "        no_overlap: If True, this ensures that no 2 top-activating tokens are in the same seq and within `buffer` of\n",
    "                    each other.\n",
    "    \"\"\"\n",
    "    assert buffer * 2 < x.size(1), \"Buffer is too large for the sequence length\"\n",
    "    assert not no_overlap or k <= x.size(0), \"Not enough sequences to have a different token in each sequence\"\n",
    "\n",
    "    if buffer > 0:\n",
    "        x = x[:, buffer:-buffer]\n",
    "\n",
    "    indices = x.flatten().argsort(-1, descending=True)\n",
    "\n",
    "    rows = indices // x.size(1)\n",
    "    cols = indices % x.size(1) + buffer\n",
    "\n",
    "    if no_overlap:\n",
    "        unique_indices = t.empty((0, 2), device=x.device).long()\n",
    "        while len(unique_indices) < k:\n",
    "            unique_indices = t.cat((unique_indices, t.tensor([[rows[0], cols[0]]], device=x.device)))\n",
    "            is_overlapping_mask = (rows == rows[0]) & ((cols - cols[0]).abs() <= buffer)\n",
    "            rows = rows[~is_overlapping_mask]\n",
    "            cols = cols[~is_overlapping_mask]\n",
    "        return unique_indices\n",
    "\n",
    "    return t.stack((rows, cols), dim=1)[:k]\n",
    "\n",
    "\n",
    "x = t.arange(40, device=device).reshape((2, 20))\n",
    "x[0, 10] += 150  # highest value\n",
    "x[0, 11] += 100  # 2nd highest value, but won't be chosen because of overlap\n",
    "x[1, 10] += 50  # 3rd highest, will be chosen\n",
    "top_indices = get_k_largest_indices(x, k=2, buffer=3)\n",
    "assert top_indices.tolist() == [[0, 10], [1, 10]]\n",
    "\n",
    "\n",
    "fetch_max_activating_examples(gpt2, gpt2_sae, gpt2_act_store, latent_idx=16873, total_batches=200, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_logits(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    latent_idx: int,\n",
    "    k: int = 10,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays the top & bottom logits for a particular latent.\n",
    "    \"\"\"\n",
    "    logits = sae.W_dec[latent_idx] @ model.W_U\n",
    "    pos_logits, pos_ids = logits.topk(k)\n",
    "    neg_logits, neg_ids = logits.topk(k, largest= False)\n",
    "    pos_tokens = model.to_str_tokens(pos_ids)\n",
    "    neg_tokens = model.to_str_tokens(neg_ids)\n",
    "\n",
    "    print(\n",
    "        tabulate(\n",
    "            zip(map(repr, neg_tokens), neg_logits, map(repr, pos_tokens), pos_logits),\n",
    "            headers=[\"Bottom tokens\", \"Value\", \"Top tokens\", \"Value\"],\n",
    "            tablefmt=\"simple_outline\",\n",
    "            stralign=\"right\",\n",
    "            numalign=\"left\",\n",
    "            floatfmt=\"+.3f\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "show_top_logits(gpt2, gpt2_sae, latent_idx=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autointerp_df(sae_release=\"gpt2-small-res-jb\", sae_id=\"blocks.7.hook_resid_pre\") -> pd.DataFrame:\n",
    "    release = get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = \"https://www.neuronpedia.org/api/explanation/export?modelId={}&saeId={}\".format(*neuronpedia_id.split(\"/\"))\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    data = response.json()\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "explanations_df = get_autointerp_df()\n",
    "explanations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 15,\n",
    "    buffer: int = 10,\n",
    ") -> dict[Literal[\"system\", \"user\", \"assistant\"], str]:\n",
    "    \"\"\"\n",
    "    Returns the system, user & assistant prompts for autointerp.\n",
    "    \"\"\"\n",
    "    data = fetch_max_activating_examples(model, sae, act_store, latent_idx, total_batches, k, buffer)\n",
    "    str_formatted_examples = \"\\n\".join(\n",
    "        f\"{i+1}. {''.join(f'<<{tok}>>' if j == buffer else tok for j, tok in enumerate(seq[1]))}\"\n",
    "        for i, seq in enumerate(data)\n",
    "    )\n",
    "    return {\n",
    "        \"system\": \"We're studying neurons in a neural network. Each neuron activates on some particular word or concept in a short document. The activating words in each document are indicated with << ... >>. Look at the parts of the document the neuron activates for and summarize in a single sentence what the neuron is activating on. Try to be specific in your explanations, although don't be so specific that you exclude some of the examples from matching your explanation. Pay attention to things like the capitalization and punctuation of the activating words or concepts, if that seems relevant. Keep the explanation as short and simple as possible, limited to 20 words or less. Omit punctuation and formatting. You should avoid giving long lists of words.\",\n",
    "        \"user\": f\"\"\"The activating documents are given below:\\n\\n{str_formatted_examples}\"\"\",\n",
    "        \"assistant\": \"this neuron fires on\",\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "# Test your function\n",
    "prompts = create_prompt(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9, total_batches=100, k=15, buffer=8)\n",
    "assert prompts[\"system\"].startswith(\"We're studying neurons in a neural network.\")\n",
    "assert \"<< new>>\" in prompts[\"user\"]\n",
    "assert prompts[\"assistant\"] == \"this neuron fires on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autointerp_explanation(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 15,\n",
    "    buffer: int = 10,\n",
    "    n_completions: int = 1,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Queries OpenAI's API using prompts returned from `create_prompt`, and returns\n",
    "    a list of the completions.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "    prompts = create_prompt(model, sae, act_store, latent_idx, total_batches, k, buffer)\n",
    "\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompts[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": prompts[\"user\"]},\n",
    "            {\"role\": \"assistant\", \"content\": prompts[\"assistant\"]},\n",
    "        ],\n",
    "        n=n_completions,\n",
    "        max_tokens=50,\n",
    "        stream=False,\n",
    "    )\n",
    "    return [choice.message.content for choice in result.choices]\n",
    "\n",
    "\n",
    "API_KEY = os.environ.get(\"OPENAI_API_KEY\", None)\n",
    "\n",
    "if API_KEY is not None:\n",
    "    completions = get_autointerp_explanation(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9, n_completions=5)\n",
    "    for i, completion in enumerate(completions):\n",
    "        print(f\"Completion {i+1}: {completion!r}\")\n",
    "else:\n",
    "    prompts = create_prompt(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9, total_batches= 100, k=15, buffer=10)\n",
    "    print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_saes = {\n",
    "    layer: SAE.from_pretrained(\n",
    "        \"gpt2-small-hook-z-kk\",\n",
    "        f\"blocks.{layer}.hook_z\",\n",
    "        device=str(device),\n",
    "    )[0]\n",
    "    for layer in range(gpt2.cfg.n_layers)\n",
    "}\n",
    "\n",
    "layer = 9\n",
    "\n",
    "display_dashboard(\n",
    "    sae_release=\"gpt2-small-hook-z-kk\",\n",
    "    sae_id=f\"blocks.{layer}.hook_z\",\n",
    "    latent_idx=2,  # or you can try `random.randint(0, attn_saes[layer].cfg.d_sae)`\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
