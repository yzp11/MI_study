import contextlib
import io

import torch as t
from sae_lens import SAE, HookedSAETransformer
from transformer_lens import HookedTransformer


def test_steering_hook(steering_hook, sae: SAE):
    steering_coefficient = 1.5
    latent_idx = 5
    activations = t.randn(1, 10, sae.cfg.d_in, device=sae.cfg.device)
    expected_result = activations.clone()
    expected_result_lastseq = activations.clone()
    result = steering_hook(activations, None, sae, latent_idx, steering_coefficient)
    assert result is not None, "Did you forget to return the tensor?"
    expected_result += steering_coefficient * sae.W_dec[latent_idx]
    expected_result_lastseq[:, -1] += steering_coefficient * sae.W_dec[latent_idx]
    assert result.shape == expected_result.shape, f"Result shape {result.shape} != expected {expected_result.shape}"
    diff = (result - expected_result).abs().max().item()
    diff_lastseq = (result - expected_result_lastseq).abs().max().item()
    if diff < 1e-5:
        print("All tests in `test_steering_hook` passed!")
    elif diff_lastseq < 1e-5:
        raise ValueError(
            "Unexpected return from steering_hook function - did you only apply steering to the last sequence position?"
        )
    else:
        raise ValueError(f"Unexpected return from steering_hook function: max diff from expected is {diff}")


def test_show_top_deembeddings(show_top_deembeddings, gpt2: HookedSAETransformer, gpt2_transcoder: SAE):
    """
    We test by checking whether a couple of expected deembeddings are in the output.
    """
    latent_idx = 1
    with io.StringIO() as buf, contextlib.redirect_stdout(buf):
        show_top_deembeddings(gpt2, gpt2_transcoder, latent_idx)
        output = buf.getvalue()
    assert "liga" in output, "Expected 'liga' to be in output (ranked highest by value)"
    assert "GAME" in output, "Expected 'GAME' to be in output (ranked second highest by value)"
    assert "jee" in output, "Expected 'jee' to be in output (ranked third highest by value)"
    print("All tests in `test_show_top_deembeddings` passed!")


def test_create_extended_embedding(create_extended_embedding, model: HookedSAETransformer):
    # Correct answer
    W_E = model.W_E.clone()[:, None, :]  # shape [batch=d_vocab, seq_len=1, d_model]
    mlp_output = model.blocks[0].mlp(model.blocks[0].ln2(W_E))  # shape [batch=d_vocab, seq_len=1, d_model]
    expected_unscaled = (W_E + mlp_output).squeeze()
    expected = (expected_unscaled - expected_unscaled.mean(dim=-1, keepdim=True)) / expected_unscaled.std(
        dim=-1, keepdim=True
    )

    # User's answer
    result = create_extended_embedding(model)

    assert result.shape == expected.shape, f"Result shape {result.shape} != expected {expected.shape}"

    diff = (result - expected).abs().max().item()
    diff_unscaled = (result - expected_unscaled).abs().max().item()

    if diff > 1e-4:
        if diff_unscaled < 1e-4:
            raise ValueError(f"Max diff from correct answer is {diff}. Did you forget to center & scale?")
        else:
            raise ValueError(f"Max diff from correct answer is {diff}.")

    print("All tests in `test_create_extended_embedding` passed!")
